{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "### Инструкция по выполнению проекта\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "### Описание данных\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 [Подготовка](#1)\n",
    "\n",
    "- [Просмотр данных](#1.1)\n",
    "- [Предварительная обработка](#1.2)\n",
    "- [Токенизация. Обработка знаков препинания и строчных букв](#1.3)\n",
    "- [Удаление стоп-слов](#1.4)\n",
    "- [Лемматизация](#1.5)\n",
    "- [Предобработка](#1.6)\n",
    "- [TF-IDF для корпуса текстов](#1.7)\n",
    "\n",
    "2 [Обучение](#2)\n",
    "\n",
    "- [LogisticRegression](#2.1)\n",
    "- [RandomForestClassifier](#2.2)\n",
    "- [BERT](#2.3)\n",
    "\n",
    "3 [Выводы](#3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка<a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Просмотр данных<a id=\"1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_tweets = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "except:\n",
    "    df_tweets = pd.read_csv('toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для скорости раскомментировать\n",
    "# df_tweets = df_tweets.sample(500).reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_tweets\n",
    "y = df_tweets['toxic']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (127656, 2) (127656,)\n",
      "test (31915, 2) (31915,)\n"
     ]
    }
   ],
   "source": [
    "print('train', X_train.shape, y_train.shape)\n",
    "print('test', X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предварительная обработка<a id=\"1.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация. Обработка знаков препинания и строчных букв<a id=\"1.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost.text_processing import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\n",
    "    lowercasing=True,\n",
    "    separator_type='BySense',\n",
    "    token_types=['Word', 'Number']\n",
    ")\n",
    "\n",
    "# def tokenize_catboost(texts):\n",
    "#    return [tokenizer.tokenize(text) for text in texts]\n",
    "\n",
    "#tokenized_catboost = tokenize_catboost(corpus_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Удаление стоп-слов<a id=\"1.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\python_dev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#def filter_stop_words(tokens):\n",
    "#    return list(filter(lambda x: x not in stop_words, tokens))\n",
    "\n",
    "#def del_stop_words(tokenized_text):\n",
    "#    return [filter_stop_words(tokens) for tokens in tokenized_text]\n",
    "    \n",
    "#tokenized_text_no_stop = del_stop_words(tokenized_catboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация<a id=\"1.5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\python_dev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens_nltk(tokens):\n",
    "    return list(map(lambda t: lemmatizer.lemmatize(t), tokens))\n",
    "\n",
    "#def lemmatize_apply(tokenized_no_stop):\n",
    "#    return [lemmatize_tokens_nltk(tokens) for tokens in tokenized_no_stop]\n",
    "#\n",
    "#text_lemmatized_nltk = lemmatize_apply(tokenized_text_no_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка<a id=\"1.6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49.2 s\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(X):\n",
    "    X_preprocessed = X.copy()\n",
    "    X_preprocessed['text'] = X['text'].apply(lambda x: ' '.join(lemmatize_tokens_nltk(tokenizer.tokenize(x))))\n",
    "    return X_preprocessed\n",
    "\n",
    "#X_lemm_train = preprocess_data(X_train)\n",
    "#X_lemm_test = preprocess_data(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF для корпуса текстов<a id=\"1.7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def tf_idf(train, test):\n",
    "    corpus_train = X_lemm_train['text'].values.astype('U')\n",
    "    corpus_test = X_lemm_test['text'].values.astype('U')\n",
    "    count_tf_idf = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "    return count_tf_idf.fit_transform(corpus_train), count_tf_idf.transform(corpus_test)\n",
    "\n",
    "    \n",
    "tf_idf_train, tf_idf_test = tf_idf(preprocess_data(X_train), preprocess_data(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение<a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = make_scorer(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression<a id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score LogisticRegression: 0.7661780479084733\n",
      "Wall time: 6.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr = LogisticRegression(C = 2.5, max_iter =200, random_state=42)\n",
    "lr.fit(tf_idf_train, y_train)\n",
    "lr_predict = lr.predict(tf_idf_test)\n",
    "\n",
    "print('f1_score LogisticRegression:', f1_score(y_test, lr_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier<a id=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {'max_depth' : [20, 30, 40],\n",
    "                'n_estimators' : [60, 80]   \n",
    "               }\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator = RandomForestClassifier(random_state=12345), \n",
    "    cv = 3,\n",
    "    param_grid = param_grid, \n",
    "    verbose = 0, \n",
    "    scoring = f1,\n",
    "    n_jobs=-1\n",
    "   \n",
    ")\n",
    "gs.fit(tf_idf_train, y_train)\n",
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score RandomForestClassifier: 0.70218474511307\n",
      "Wall time: 13min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "forest_model = RandomForestClassifier(random_state=12345)\n",
    "forest_model.fit(tf_idf_train, y_train)\n",
    "forest_predict = forest_model.predict(tf_idf_test)\n",
    "print('f1_score RandomForestClassifier:', f1_score(y_test, forest_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "    batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)])\n",
    "    attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "\n",
    "    embeddings.append(batch_embeddings[0][:,0,:].numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT <a id=\"2.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка на адекватность "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score random_mean: 0.09804520464263898\n"
     ]
    }
   ],
   "source": [
    "share = y_test.mean()\n",
    "predictions_random_mean = pd.Series(\n",
    "    random.choices(\n",
    "        [1, 0], \n",
    "        weights=[share, 1 - share], \n",
    "        k=len(y_test)), \n",
    "    index=y_test.index)\n",
    "print('f1_score random_mean:', f1_score(y_test, predictions_random_mean))\n",
    "#print(predictions_random_mean.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "результат подозрительный. смущает"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [ ]  Весь код выполняется без ошибок\n",
    "- [ ]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [ ]  Данные загружены и подготовлены\n",
    "- [ ]  Модели обучены\n",
    "- [ ]  Значение метрики *F1* не меньше 0.75\n",
    "- [ ]  Выводы написаны"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### CatBoostClassifier\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from tqdm import notebook\n",
    "import torch\n",
    "\n",
    "def predict_model(X_train, y_train, X_test, **kwargs):\n",
    "    CatBoost = CatBoostClassifier(eval_metric='F1', iterations=300, learning_rate=0.05, od_type='Iter', **kwargs)\n",
    "    CatBoost.fit(X_train, y_train, verbose=100)\n",
    "    return CatBoost.predict(X_test)\n",
    "\n",
    "predict = predict_model(tf_idf_train, y_train, tf_idf_test)\n",
    "#\n",
    "#f1_score(y_test, predict, average='weighted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
