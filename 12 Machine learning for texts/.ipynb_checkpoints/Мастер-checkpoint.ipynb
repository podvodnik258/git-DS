{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Привет, Василий! \n",
    "\n",
    "Меня зовут Светлана Медведева и я буду проверять Твою работу. Предлагаю общаться на \"ты\". \n",
    "\n",
    "Просьба при доработке работы оставлять мои комментарии без изменений.\n",
    "\n",
    "Комментарии я разделяю на следующие категории:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "В случае если всё верно!\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "В случае если можно что-то доработать, но эта доработка не критична или если есть варианты улучшения работы.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "Замечания, которые нужно исправить. Без исправления этих замечаний проект принят не может быть.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Резензия на работу</font>\n",
    "* К сожалению, я не могу запустить на сервере Твою работу. Проект падает с ошибкой \"The kernel appears to have died. It will restart automatically.\". Предлагаю попробовать вместо TfidfVectorizer другой метод."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "### Инструкция по выполнению проекта\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "### Описание данных\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 [Подготовка](#1)\n",
    "\n",
    "- [Просмотр данных](#1.1)\n",
    "- [Анализ данных](#1.1.1)\n",
    "- [Подготовка данных](#1.2.1)\n",
    "- [Предварительная обработка](#1.2)\n",
    "- [Токенизация. Обработка знаков препинания и строчных букв](#1.3)\n",
    "- [Удаление стоп-слов](#1.4)\n",
    "- [Лемматизация](#1.5)\n",
    "- [Создание корпусов текста](#1.6)\n",
    "- [Векторизация текста](#1.6.1)\n",
    "- [Регулярные выражения. Мешок слов и N-граммы](#1.6.2)\n",
    "- [TF-IDF для корпуса текстов](#1.7)\n",
    "- [Выбор метода векторизации](#1.7.1)\n",
    "\n",
    "2 [Обучение](#2)\n",
    "- [Метрика](#2.0.1)\n",
    "- [LogisticRegression](#2.1)\n",
    "- [RandomForestClassifier](#2.2)\n",
    "- [CatBoostClassifier](#2.2.1)\n",
    "- [BERT](#2.3)\n",
    "- [Выборка для BERT](#2.3.0)\n",
    "- [Загрузка предварительно обученной модели DistilBert](#2.3.1)\n",
    "- [Подготовка набора данных](#2.3.2)\n",
    "- [Тестирование DistilBert моделью LogisticRegression](#2.3.3)\n",
    "- [Проверка на адекватность](#2.3.4)\n",
    "\n",
    "3 [Выводы](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Комментарий ревьюера v. 1:\n",
    "    \n",
    "Молодец, что добавил оглавление со ссылками и кратко описал проект.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "import torch\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from catboost import CatBoostClassifier\n",
    "from catboost.text_processing import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import transformers as trsfrs\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка<a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Просмотр данных<a id=\"1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_tweets = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "except:\n",
    "    df_tweets = pd.read_csv('toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Info ==========\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    100000 non-null  object\n",
      " 1   toxic   100000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== missing data ==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Total  Percent\n",
       "toxic      0      0.0\n",
       "text       0      0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== duplicated ==========\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== describe ==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.101330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.301767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic\n",
       "count  100000.000000\n",
       "mean        0.101330\n",
       "std         0.301767\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.000000\n",
       "max         1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== inspection ==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\" onto others by pasting this code into their ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>. Verily that latter article's</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"\\n\\n  Celebrations for 2nd Anniversary of Wik...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tyra [Juola] Szpara has been named one of the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biographical info \\n\\nMr. Kee supplied this in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>You are such an ignorant piece of refuse. You ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I will not answer to your provaction anymore.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I understand your logic, but I disagreed with ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a fight and argument'''. Also, I am very sting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"\\nIt was \"\"British and Irish Lions\"\" in the 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  \" onto others by pasting this code into their ...      0\n",
       "1                     . Verily that latter article's      0\n",
       "2  \"\\n\\n  Celebrations for 2nd Anniversary of Wik...      0\n",
       "3  Tyra [Juola] Szpara has been named one of the ...      0\n",
       "4  Biographical info \\n\\nMr. Kee supplied this in...      0\n",
       "5  You are such an ignorant piece of refuse. You ...      1\n",
       "6      I will not answer to your provaction anymore.      0\n",
       "7  I understand your logic, but I disagreed with ...      0\n",
       "8  a fight and argument'''. Also, I am very sting...      1\n",
       "9  \"\\nIt was \"\"British and Irish Lions\"\" in the 1...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def missing_data(data):\n",
    "    total = data.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n",
    "    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "\n",
    "def preview(data):\n",
    "    print('=='*5, 'Info' , '=='*5)\n",
    "    display(data.info())\n",
    "    print('=='*5, 'missing data', '=='*5)\n",
    "    display(missing_data(data))\n",
    "    print('=='*5, 'duplicated', '=='*5)\n",
    "    display(data.duplicated().sum())\n",
    "    print('=='*5, 'describe', '=='*5)\n",
    "    display(data.describe())\n",
    "    print('=='*5, 'inspection', '=='*5)\n",
    "    display(data.head(10))\n",
    "\n",
    "preview(df_tweets)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ данных <a id=\"1.1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_percent(data):\n",
    "    total = data.value_counts()\n",
    "    percent = (data.value_counts() / data.value_counts().sum()*100)\n",
    "    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent']).head(5)\n",
    "\n",
    "def target_analize(target_column):\n",
    "    sns.countplot(target_column)\n",
    "    display(get_top_percent(target_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89867</td>\n",
       "      <td>89.867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10133</td>\n",
       "      <td>10.133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total  Percent\n",
       "0  89867   89.867\n",
       "1  10133   10.133"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_analize(df_tweets['toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Комментарий ревьюера v. 1:\n",
    "    \n",
    "Проведи, пожалуйста, исследовательский анализ данных.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных<a id=\"1.2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для скорости раскомментировать\n",
    "df_tweets = df_tweets.sample(100000).reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_tweets\n",
    "y = df_tweets['toxic']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (80000, 2) (80000,)\n",
      "test (20000, 2) (20000,)\n"
     ]
    }
   ],
   "source": [
    "print('train', X_train.shape, y_train.shape)\n",
    "print('test', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предварительная обработка<a id=\"1.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация. Обработка знаков препинания и строчных букв<a id=\"1.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost.text_processing import Tokenizer\n",
    "tokenizer = Tokenizer(\n",
    "    lowercasing=True,\n",
    "    separator_type='BySense',\n",
    "    token_types=['Word', 'Number']\n",
    ")\n",
    "\n",
    "# def tokenize_catboost(texts):\n",
    "#    return [tokenizer.tokenize(text) for text in texts]\n",
    "\n",
    "#tokenized_catboost = tokenize_catboost(corpus_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Удаление стоп-слов<a id=\"1.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\python_dev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#def filter_stop_words(tokens):\n",
    "#    return list(filter(lambda x: x not in stop_words, tokens))\n",
    "\n",
    "#def del_stop_words(tokenized_text):\n",
    "#    return [filter_stop_words(tokens) for tokens in tokenized_text]\n",
    "    \n",
    "#tokenized_text_no_stop = del_stop_words(tokenized_catboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация<a id=\"1.5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\python_dev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens_nltk(tokens):\n",
    "    return list(map(lambda t: lemmatizer.lemmatize(t), tokens))\n",
    "\n",
    "#def lemmatize_apply(tokenized_no_stop):\n",
    "#    return [lemmatize_tokens_nltk(tokens) for tokens in tokenized_no_stop]\n",
    "#\n",
    "#text_lemmatized_nltk = lemmatize_apply(tokenized_text_no_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание корпусов текста<a id=\"1.6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_train (80000,)\n",
      "corpus_test (20000,)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(X):\n",
    "    X_preprocessed = X.copy()\n",
    "    X_preprocessed['text'] = X['text'].apply(lambda x: ' '.join(lemmatize_tokens_nltk(tokenizer.tokenize(x))))\n",
    "    return X_preprocessed\n",
    "\n",
    "corpus_train = preprocess_data(X_train)['text'].values.astype('U')\n",
    "corpus_test = preprocess_data(X_test)['text'].values.astype('U')\n",
    "\n",
    "print('corpus_train', corpus_train.shape)\n",
    "print('corpus_test', corpus_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Комментарий ревьюера v. 1:\n",
    "    \n",
    "Молодец, что используешь TfidfVectorizer, но попробуй, пожалуйста, убрать (закомментировать) этот метод из проекта, т. к. предположительно из-за него код падает на сервере с ошибкой (\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторизация текста<a id=\"1.6.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регулярные выражения. Мешок слов и N-граммы<a id=\"1.6.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    substitute = re.sub(r'[^A-Za-z]', ' ', text)\n",
    "    substitute = substitute.split()\n",
    "    substitute = ' '.join(substitute)\n",
    "    return substitute\n",
    "\n",
    "def clear_text_apply(corpus):\n",
    "    return [clear_text(text) for text in corpus]\n",
    "\n",
    "# создание n-граммы n_gramm, для которой n=1\n",
    "def n_gramm(corpus_train, corpus_test):\n",
    "    count_vect = CountVectorizer(analyzer='word', ngram_range=(1, 1), stop_words=stop_words) \n",
    "    n_gramm_train = count_vect.fit_transform(clear_text_apply(corpus_train))\n",
    "    n_gramm_test = count_vect.transform(clear_text_apply(corpus_test))\n",
    "    \n",
    "    return n_gramm_train, n_gramm_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF для корпуса текстов<a id=\"1.7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(corpus_train, corpus_test):\n",
    "    count_tf_idf = TfidfVectorizer(stop_words=stop_words)\n",
    "    tf_idf_train = count_tf_idf.fit_transform(corpus_train)\n",
    "    tf_idf_test = count_tf_idf.transform(corpus_test)\n",
    "    \n",
    "    return  tf_idf_train, tf_idf_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор метода векторизации<a id=\"1.7.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы vector_train: (80000, 104585)\n",
      "Размер матрицы vector_test: (20000, 104585)\n",
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 0 если используется N-grams \n",
    "# 1 если используется TF-IDF\n",
    "\n",
    "TEXT_VECTORIZATION_MODEL = 0\n",
    "\n",
    "if TEXT_VECTORIZATION_MODEL == 0:\n",
    "    vector_train, vector_test = n_gramm(corpus_train, corpus_test)\n",
    "elif TEXT_VECTORIZATION_MODEL == 1:\n",
    "    vector_train, vector_test = tf_idf(corpus_train, corpus_test)\n",
    "\n",
    "    \n",
    "    \n",
    "print(\"Размер матрицы vector_train:\", vector_train.shape)\n",
    "print(\"Размер матрицы vector_test:\", vector_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение<a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метрика<a id=\"2.0.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = make_scorer(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение моделей\n",
    "def f1_score_model(model, X = vector_train, y = y_train, X_test = vector_test, y_test = y_test):\n",
    "    model.fit(X, y)\n",
    "    predict = model.predict(X_test)\n",
    "\n",
    "    print('f1_score test:', f1_score(y_test, predict))\n",
    "    \n",
    "# GridSearch\n",
    "def searchCV(estimator, param_grid, X, y):\n",
    "    gs = GridSearchCV(estimator=estimator,\n",
    "                          param_grid=param_grid,\n",
    "                          cv= 3,\n",
    "                          verbose = 0, \n",
    "                          scoring = f1,\n",
    "                          n_jobs=-1)\n",
    "    gs.fit(X, y)\n",
    "    print('best scrores GridSearch: ', gs.best_score_)\n",
    "    print('best scrores: ', gs.best_params_)\n",
    "    \n",
    "    return gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression<a id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best scrores GridSearch:  0.7467895094578799\n",
      "best scrores:  {'C': 1.0, 'max_iter': 100.0}\n",
      "f1_score test: 0.7602052197189382\n"
     ]
    }
   ],
   "source": [
    "estimator = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "param_grid = {\n",
    "    'C': np.linspace(1, 3, 2), \n",
    "    'max_iter':np.linspace(100, 200, 1)\n",
    "}\n",
    "\n",
    "f1_score_model(searchCV(estimator, param_grid, vector_train, y_train)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier<a id=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best scrores GridSearch:  0.03495132292757656\n",
    "\n",
    "best scrores:  {'max_depth': 40, 'n_estimators': 80}\n",
    "\n",
    "f1_score test: 0.02856274688544515"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RandomForestClassifier(random_state=12345)\n",
    "param_grid = {\n",
    "    'max_depth' : [30, 40],\n",
    "    'n_estimators' : [60, 80]   \n",
    "}\n",
    "\n",
    "f1_score_model(searchCV(estimator, param_grid, vector_train, y_train)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoostClassifier<a id=\"2.2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1_score CatBoost BERT: 0.5170556552962299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(X_train, y_train, X_valid, **kwargs):\n",
    "    CatBoost = CatBoostClassifier(eval_metric='F1', iterations=300, learning_rate=0.01, od_type='Iter', **kwargs)\n",
    "    CatBoost.fit(X_train, y_train, verbose=50)\n",
    "    return CatBoost.predict(X_valid)\n",
    "\n",
    "predict = predict_model(vector_train, y_train, vector_test)\n",
    "#\n",
    "print('f1_score CatBoost BERT:', f1_score(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT <a id=\"2.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Комментарий ревьюера v. 1:\n",
    "    \n",
    "Доделай, пожалуйста, проект.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выборка для BERT<a id=\"2.3.0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы не создавать эмбеддинги слишком долго, возьмем из выборки только 400 случайных элементов. Для корректного тестирования поделим их на обучающую и тестовую выборки в соотношении 50:50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_tweets.sample(400, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bert, test_bert = train_test_split(df,  test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка предварительно обученной модели DistilBert<a id=\"2.3.1\"></a>\n",
    "модель переменных DistilBert содержит предварительно обученную модель BERT, которая меньше, но намного быстрее и требует гораздо меньше памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (trsfrs.DistilBertModel, trsfrs.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## для BERT:\n",
    "#model_class, tokenizer_class, pretrained_weights = (trsfrs.BertModel, trsfrs.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Загрузка предварительно подготовленных model/tokenizer\n",
    "token = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка набора данных<a id=\"2.3.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#определяем размер батча для обработки BERT-ом кратно выборке\n",
    "BATCH_SIZE = 40  \n",
    "\n",
    "def bert(text):\n",
    "    # Tokenization/ Ограничим длинну твита в 512, по условию модели\n",
    "    tokenized = text.apply((lambda x: token.encode(x, add_special_tokens=True, max_length=512)))\n",
    "\n",
    "    # Padding\n",
    "    # вычисляем максимальное кол-во токенов в предложении\n",
    "    max_len = 0\n",
    "    for i in tokenized.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "        \n",
    "    #добавляем 0 для строк, у которых длина меньше максимальной\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "    np.array(padded).shape\n",
    "\n",
    "    # Masking\n",
    "    # создаем маску, чтобы выделить значимые токены\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    # attention_mask.shape\n",
    "\n",
    "    # BATCH_SIZE = 40  #определяем размер батча для обработки BERT-ом\n",
    "    embeddings = []\n",
    "    for i in notebook.tqdm(range(padded.shape[0] // BATCH_SIZE)):\n",
    "        batch = torch.LongTensor(padded[BATCH_SIZE*i:BATCH_SIZE*(i+1)]) \n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[BATCH_SIZE*i:BATCH_SIZE*(i+1)])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].numpy())\n",
    "        \n",
    "    #полученные эмбеддинги будут признаками для обучения модели\n",
    "    bert_features = np.concatenate(embeddings) \n",
    "    print('shape:', bert_features.shape)\n",
    "    return bert_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert_train = bert(train_bert['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert_test = bert(test_bert['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование  DistilBert моделью LogisticRegression<a id=\"2.3.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "param_grid = {\n",
    "    'C': np.linspace(1, 8, 4), \n",
    "    'max_iter':np.linspace(100, 200, 3)\n",
    "}\n",
    "\n",
    "f1_score_model(searchCV(estimator, param_grid, X_bert_train, train_bert['toxic']),X_bert_train, train_bert['toxic'], X_bert_test, test_bert['toxic']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка на адекватность<a id=\"2.3.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = DummyClassifier()\n",
    "param_grid = {}\n",
    "\n",
    "f1_score_model(searchCV(estimator, param_grid, vector_train, y_train)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "результат подозрительный. смущает"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Комментарий ревьюера v. 1:\n",
    "    \n",
    "Пока не могу посмотреть результаты, не запускается код (\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [ ]  Весь код выполняется без ошибок\n",
    "- [ ]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [ ]  Данные загружены и подготовлены\n",
    "- [ ]  Модели обучены\n",
    "- [ ]  Значение метрики *F1* не меньше 0.75\n",
    "- [ ]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
