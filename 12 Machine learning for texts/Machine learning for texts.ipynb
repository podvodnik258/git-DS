{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "### Описание данных\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 [Подготовка](#1)\n",
    "\n",
    "- [Просмотр данных](#1.1)\n",
    "- [Анализ данных](#1.1.1)\n",
    "- [Подготовка данных](#1.2.1)\n",
    "- [Предварительная обработка](#1.2)\n",
    "- [Токенизация. Обработка знаков препинания и строчных букв](#1.3)\n",
    "- [Удаление стоп-слов](#1.4)\n",
    "- [Лемматизация](#1.5)\n",
    "- [Создание корпусов текста](#1.6)\n",
    "- [Векторизация текста](#1.6.1)\n",
    "- [Регулярные выражения. Мешок слов и N-граммы](#1.6.2)\n",
    "- [TF-IDF для корпуса текстов](#1.7)\n",
    "- [Выбор метода векторизации](#1.7.1)\n",
    "\n",
    "2 [Обучение](#2)\n",
    "- [Метрика](#2.0.1)\n",
    "- [LogisticRegression](#2.1)\n",
    "- [RandomForestClassifier](#2.2)\n",
    "- [CatBoostClassifier](#2.2.1)\n",
    "- [BERT](#2.3)\n",
    "- [Выборка для BERT](#2.3.0)\n",
    "- [Загрузка предварительно обученной модели DistilBert](#2.3.1)\n",
    "- [Подготовка набора данных](#2.3.2)\n",
    "- [Тестирование DistilBert моделью LogisticRegression](#2.3.3)\n",
    "- [Проверка на адекватность](#2.3.4)\n",
    "\n",
    "3 [Выводы](#3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "import torch\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from catboost import CatBoostClassifier\n",
    "from catboost.text_processing import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import transformers as trsfrs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка<a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Просмотр данных<a id=\"1.1\"></a>\n",
    "Загружаем данные, смотрим особенности датасета функцией `preview`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_tweets = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "except:\n",
    "    df_tweets = pd.read_csv('toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Info ==========\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== missing data ==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Total  Percent\n",
       "toxic      0      0.0\n",
       "text       0      0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== duplicated ==========\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== describe ==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <td>159571.0</td>\n",
       "      <td>0.101679</td>\n",
       "      <td>0.302226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count      mean       std  min  25%  50%  75%  max\n",
       "toxic  159571.0  0.101679  0.302226  0.0  0.0  0.0  0.0  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== inspection ==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0\n",
       "5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7  Your vandalism to the Matt Shirvington article...      0\n",
       "8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9  alignment on this subject and which are contra...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def missing_data(data):\n",
    "    total = data.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n",
    "    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "\n",
    "def preview(data):\n",
    "    print('=='*5, 'Info' , '=='*5)\n",
    "    display(data.info())\n",
    "    print('=='*5, 'missing data', '=='*5)\n",
    "    display(missing_data(data))\n",
    "    print('=='*5, 'duplicated', '=='*5)\n",
    "    display(data.duplicated().sum())\n",
    "    print('=='*5, 'describe', '=='*5)\n",
    "    display(data.describe().T)\n",
    "    print('=='*5, 'inspection', '=='*5)\n",
    "    display(data.head(10))\n",
    "\n",
    "preview(df_tweets)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ данных <a id=\"1.1.1\"></a>\n",
    "Изучим особенности целевого признака"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_percent(data):\n",
    "    total = data.value_counts()\n",
    "    percent = (data.value_counts() / data.value_counts().sum()*100)\n",
    "    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent']).head(5)\n",
    "\n",
    "def target_analize(target_column):\n",
    "    sns.countplot(target_column)\n",
    "    display(get_top_percent(target_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>143346</td>\n",
       "      <td>89.832112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16225</td>\n",
       "      <td>10.167888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Total    Percent\n",
       "0  143346  89.832112\n",
       "1   16225  10.167888"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_analize(df_tweets['toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токсичных комментариев в датасете 10%. Датасет не сбалансирован."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных<a id=\"1.2.1\"></a>\n",
    "Так, как имеется ограничения в памяти, то имеется возможность ограничить датасет, взяв только часть наблюдений.\n",
    "\n",
    "Создаем обучающие выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для стабильности раскомментировать\n",
    "df_tweets = df_tweets.sample(100000, random_state=42).reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_tweets\n",
    "y = df_tweets['toxic']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (80000, 2) (80000,)\n",
      "test (20000, 2) (20000,)\n"
     ]
    }
   ],
   "source": [
    "print('train', X_train.shape, y_train.shape)\n",
    "print('test', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предварительная обработка<a id=\"1.2\"></a>\n",
    "Определляем Токенайзер, файл стоп-слов, проводим лемматизацию, создаём корпус твитов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация. Обработка знаков препинания и строчных букв<a id=\"1.3\"></a>\n",
    " Процесс извлечения токенов-слов, цифр, знаков препинания или специальных символов, которые определяют эмодзи из последовательности, называется токенизацией.Выполняется как простое разбиение последовательности на строковый шаблон (например, пробел)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost.text_processing import Tokenizer\n",
    "tokenizer = Tokenizer(\n",
    "    lowercasing=True,\n",
    "    separator_type='BySense',\n",
    "    token_types=['Word'] # , 'Number'\n",
    ")\n",
    "\n",
    "# def tokenize_catboost(texts):\n",
    "#    return [tokenizer.tokenize(text) for text in texts]\n",
    "\n",
    "#tokenized_catboost = tokenize_catboost(corpus_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Удаление стоп-слов<a id=\"1.4\"></a>\n",
    "Стоп - слова -слова, которые считаются неинформативными в этой задаче, например функциональные слова, такие как, is, at, which, on. Обычно стоп-слова удаляются во время предварительной обработки текста, чтобы уменьшить объем информации, которая рассматривается для дальнейших алгоритмов. Стоп-слова собираются вручную (в виде словаря) или автоматически, например, беря наиболее частые слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     D:\\Users\\python_dev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#def filter_stop_words(tokens):\n",
    "#    return list(filter(lambda x: x not in stop_words, tokens))\n",
    "\n",
    "#def del_stop_words(tokenized_text):\n",
    "#    return [filter_stop_words(tokens) for tokens in tokenized_text]\n",
    "    \n",
    "#tokenized_text_no_stop = del_stop_words(tokenized_catboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация<a id=\"1.5\"></a>\n",
    "Лемма (Википедия) - это каноническая форма, словарная форма или форма цитирования набора слов. Например, Лемма \"go\" представляет собой формы \"go\", \"goes\", \"going\", \"went\", and \"gone\". Процесс преобразования слова в его лемму называется лемматизация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     D:\\Users\\python_dev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens_nltk(tokens):\n",
    "    return list(map(lambda t: lemmatizer.lemmatize(t), tokens))\n",
    "\n",
    "#def lemmatize_apply(tokenized_no_stop):\n",
    "#    return [lemmatize_tokens_nltk(tokens) for tokens in tokenized_no_stop]\n",
    "#\n",
    "#text_lemmatized_nltk = lemmatize_apply(tokenized_text_no_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание корпусов текста<a id=\"1.6\"></a>\n",
    "Чтобы алгоритмы умели определять тематику и тональность текста, их нужно обучить на корпусе (англ. corpus). Это набор текстов, в котором эмоции и ключевые слова уже размечены.\n",
    "Создадим корпус постов. Преобразуем столбец `text` в список текстов. Переведём тексты в стандартный для Python формат: кодировку `Unicode (U)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_train (80000,)\n",
      "corpus_test (20000,)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(X):\n",
    "    X_preprocessed = X.copy()\n",
    "    X_preprocessed['text'] = X['text'].apply(lambda x: ' '.join(lemmatize_tokens_nltk(tokenizer.tokenize(x))))\n",
    "    return X_preprocessed\n",
    "\n",
    "corpus_train = preprocess_data(X_train)['text'].values.astype('U')\n",
    "corpus_test = preprocess_data(X_test)['text'].values.astype('U')\n",
    "\n",
    "print('corpus_train', corpus_train.shape)\n",
    "print('corpus_test', corpus_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"is the disney example really demonstration of an imaginary colour to me it sound more like that old optical illusion where a checkerboard ha a shadow thrown on it such that the same middle shade of gray look darker and lighter depending on whether it's surrounded by bright or by dark colour a in the grass look greener because it's more different from pink than because the pink is fatiguing your red and blue receptor\",\n",
       "       'no a i do not believe the other side to be contributing in earnest my rhetoric is well-justified it is up to people like you to make a stand but you continue to fail the community and the encyclopaedia',\n",
       "       \"purpose this rfc will be used to determine community consensus for outline potential resource to gain information wikipedia talk outlines# point user talk dbachmann# more on outline possibly this should be extended to be a discussion of navigation article within mainspace taking from s post on dbachmann's talk page i can see broadening this to include index and list of x topic in my opinion glossary timeline and year article may be useful for navigation but they also have the capability to become complete article used for purpose other than navigation i don't see outline index and list of x topic article a being able to be more than a navigational aid thought\"],\n",
       "      dtype='<U5000')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_train[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторизация текста<a id=\"1.6.1\"></a>\n",
    "Преобразование текста в векторы фиксированного размера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регулярные выражения. Мешок слов и N-граммы<a id=\"1.6.2\"></a>\n",
    "От лишних символов текст очистят регулярные выражения (англ. regular expressions). Это инструмент для поиска слова или числа по шаблону (англ. pattern). Он определяет, из каких частей состоит строка и какие в них символы. Например, нужно найти все даты в таком формате записи: 02.12.2020. Их шаблон — это два числа, точка, два числа, точка, четыре числа. Возьмём исходный текст твита. Под наш шаблон подходят английские символы и пробелы, которые как раз нужно оставить. Но если мы вызовем функцию re.sub(), их заменят пробелы. Чтобы указать, что символы под шаблон не подходят, перед набором символов поставим знак «домика» (^). Так в тексте остались только кириллические символы и пробелы. После этой операции в тексте можно обнаружить лишние пробелы, для анализа они — помеха. Пробелы устраняются комбинацией функций join() и split().\n",
    "Возьмём пример текста с лишними пробелами: в середине, начале и конце строки. Методом split() преобразуем его в список. Если не указывать аргументы у split(), он делает разбиение по пробелам или группам пробелов:\n",
    "\n",
    "Модель «мешок слов» (англ. bag of words) преобразует текст в вектор, не учитывая порядок слов. Отсюда и название — «мешок».\n",
    "Когда текстов несколько, мешок слов преобразует их в матрицу. Её строки — это тексты, а столбцы — уникальные слова из всех текстов корпуса. Числа на пересечении строк и столбцов показывают, сколько раз в тексте встречается уникальное слово. В мешке слов учитывается каждое уникальное слово. Но порядок слов и связи между ними не учитываются.\n",
    "\n",
    "N-грамма — это последовательность из нескольких слов. N указывает на количество элементов и может быть любым. Например, если N равно 1, получаются слова, или униграммы (лат. unus, «один»). При N=2 выходят словосочетания из двух слов — биграммы (лат. bis, «дважды»). Если N=3, то это уже триграммы (лат. tres, **«три»), т. е. из трёх слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    substitute = re.sub(r'[^A-Za-z]', ' ', text)\n",
    "    substitute = substitute.split()\n",
    "    substitute = ' '.join(substitute)\n",
    "    return substitute\n",
    "\n",
    "def clear_text_apply(corpus):\n",
    "    return [clear_text(text) for text in corpus]\n",
    "\n",
    "# создание n-граммы n_gramm, для которой n=1\n",
    "def n_gramm(corpus_train, corpus_test):\n",
    "    count_vect = CountVectorizer(analyzer='word', ngram_range=(1, 1), stop_words=stop_words) \n",
    "    n_gramm_train = count_vect.fit_transform(clear_text_apply(corpus_train))\n",
    "    n_gramm_test = count_vect.transform(clear_text_apply(corpus_test))\n",
    "    \n",
    "    return n_gramm_train, n_gramm_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF для корпуса текстов<a id=\"1.7\"></a>\n",
    "Оценка важности слова определяется величиной TF-IDF (от англ. term frequency, «частота терма, или слова»; inverse document frequency, «обратная частота документа, или текста»). То есть TF отвечает за количество упоминаний слова в отдельном тексте, а IDF отражает частоту его употребления во всём корпусе. Большая величина TF-IDF говорит об уникальности слова в тексте по отношению к корпусу. Чем чаще оно встречается в конкретном тексте и реже в остальных, тем выше значение TF-IDF\n",
    "\n",
    "- $TF$ термина $а$ = (Количество раз, когда термин $а$ встретился в тексте / количество всех слов в тексте)\n",
    "\n",
    "$$\n",
    "TF = t/n, \n",
    "$$\n",
    "\n",
    "где $t$ (от англ. term) — количество употребления слова, а $n$ — общее число слов в тексте.\n",
    "\n",
    "\n",
    "- $IDF$ термина $а$ = логарифм(Общее количество документов / Количество документов, в которых встречается термин $а$)\n",
    "\n",
    "$$\n",
    "IDF = log10(D/d),\n",
    "$$\n",
    "\n",
    "где $D$ общее число текстов в корпусе и $d$ - количество текстов, в которых это слово встречается.\n",
    "\n",
    "\n",
    "- $TF-IDF$ термина $а$ = ($TF$ термина $а$) * ($IDF$ термина $а$)\n",
    "\n",
    "$$\n",
    "TF-IDF = TF*IDF\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(corpus_train, corpus_test):\n",
    "    count_tf_idf = TfidfVectorizer(stop_words=stop_words)\n",
    "    tf_idf_train = count_tf_idf.fit_transform(corpus_train)\n",
    "    tf_idf_test = count_tf_idf.transform(corpus_test)\n",
    "    \n",
    "    return  tf_idf_train, tf_idf_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор метода векторизации<a id=\"1.7.1\"></a>\n",
    "Здесь можно выбрать какой метод векторизации нужно использовать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы vector_train: (80000, 106545)\n",
      "Размер матрицы vector_test: (20000, 106545)\n",
      "Wall time: 5.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 0 если используется N-grams \n",
    "# 1 если используется TF-IDF\n",
    "\n",
    "TEXT_VECTORIZATION_MODEL = 1\n",
    "\n",
    "if TEXT_VECTORIZATION_MODEL == 0:\n",
    "    vector_train, vector_test = n_gramm(corpus_train, corpus_test)\n",
    "elif TEXT_VECTORIZATION_MODEL == 1:\n",
    "    vector_train, vector_test = tf_idf(corpus_train, corpus_test)\n",
    "\n",
    "    \n",
    "    \n",
    "print(\"Размер матрицы vector_train:\", vector_train.shape)\n",
    "print(\"Размер матрицы vector_test:\", vector_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение<a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метрика<a id=\"2.0.1\"></a>\n",
    "создаем метрику для использования в поиске гиперпараметров GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = make_scorer(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функции для подбора параметров, и вывода результата. На вход принимает модель, словарь с подбираемыми гиперпараметрами, данные для обучения. Возвращает f1_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение моделей\n",
    "def f1_score_model(model, X = vector_train, y = y_train, X_test = vector_test, y_test = y_test):\n",
    "    model.fit(X, y)\n",
    "    predict = model.predict(X_test)\n",
    "\n",
    "    print('f1_score test:', f1_score(y_test, predict))\n",
    "    \n",
    "# GridSearch\n",
    "def searchCV(estimator, param_grid, X, y):\n",
    "    gs = GridSearchCV(estimator=estimator,\n",
    "                          param_grid=param_grid,\n",
    "                          cv= 3,\n",
    "                          verbose = 0, \n",
    "                          scoring = f1,\n",
    "                          n_jobs=-1)\n",
    "    gs.fit(X, y)\n",
    "    print('best scrores GridSearch: ', gs.best_score_)\n",
    "    print('best scrores: ', gs.best_params_)\n",
    "    \n",
    "    return gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression<a id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best scrores GridSearch:  0.7509459753306457\n",
      "best scrores:  {'C': 3, 'max_iter': 100.0}\n",
      "f1_score test: 0.7575965273018049\n",
      "Wall time: 28.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator = LogisticRegression(random_state=12345, class_weight='balanced') #\n",
    "param_grid = {\n",
    "    'C': [1, 2.5, 3], \n",
    "    'max_iter':np.linspace(100, 200, 2)\n",
    "}\n",
    "\n",
    "f1_score_model(searchCV(estimator, param_grid, vector_train, y_train)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее модели RandomForestClassifier, CatBoostClassifier, DistilBert были посчитаны. К сожалению используемая память, видимо, превышает лимиты, и результаты вычислений я оставил перед расчетами. Сами расчеты не выполняются, находятся в формате raw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier<a id=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best scrores GridSearch:  0.04151597872956869\n",
      "best scrores:  {'max_depth': 40, 'n_estimators': 60}\n",
      "f1_score test: 0.04934687953555878\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimator = RandomForestClassifier(random_state=12345)\n",
    "param_grid = {\n",
    "    'max_depth' : [30, 40],\n",
    "    'n_estimators' : [60, 80]   \n",
    "}\n",
    "\n",
    "f1_score_model(searchCV(estimator, param_grid, vector_train, y_train)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoostClassifier<a id=\"2.2.1\"></a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "def predict_model(X_train, y_train, X_valid, **kwargs):\n",
    "    CatBoost = CatBoostClassifier(eval_metric='F1', iterations=300, learning_rate=0.01, od_type='Iter', **kwargs)\n",
    "    CatBoost.fit(X_train, y_train, verbose=50)\n",
    "    return CatBoost.predict(X_valid)\n",
    "\n",
    "predict = predict_model(vector_train, y_train, vector_test)\n",
    "#\n",
    "print('f1_score CatBoostClassifier:', f1_score(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT <a id=\"2.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выборка для BERT<a id=\"2.3.0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы не создавать эмбеддинги слишком долго, возьмем из выборки только 400 случайных элементов. Для корректного тестирования поделим их на обучающую и тестовую выборки в соотношении 50:50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_tweets.sample(400, random_state=42).reset_index(drop=True)\n",
    "train_bert, test_bert = train_test_split(df,  test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка предварительно обученной модели DistilBert<a id=\"2.3.1\"></a>\n",
    "модель переменных DistilBert содержит предварительно обученную модель BERT, которая меньше, но намного быстрее и требует гораздо меньше памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (trsfrs.DistilBertModel, trsfrs.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## для BERT:\n",
    "#model_class, tokenizer_class, pretrained_weights = (trsfrs.BertModel, trsfrs.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Загрузка предварительно подготовленных model/tokenizer\n",
    "token = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка набора данных<a id=\"2.3.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#определяем размер батча для обработки BERT-ом кратно выборке\n",
    "BATCH_SIZE = 20  \n",
    "\n",
    "def bert(text):\n",
    "    # Tokenization/ Ограничим длинну твита в 512, по условию модели\n",
    "    tokenized = text.apply((lambda x: token.encode(x, add_special_tokens=True, max_length=512)))\n",
    "\n",
    "    # Padding\n",
    "    # вычисляем максимальное кол-во токенов в предложении\n",
    "    max_len = 0\n",
    "    for i in tokenized.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "        \n",
    "    #добавляем 0 для строк, у которых длина меньше максимальной\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "    np.array(padded).shape\n",
    "\n",
    "    # Masking\n",
    "    # создаем маску, чтобы выделить значимые токены\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    # attention_mask.shape\n",
    "\n",
    "    # BATCH_SIZE = 40  #определяем размер батча для обработки BERT-ом\n",
    "    embeddings = []\n",
    "    for i in notebook.tqdm(range(padded.shape[0] // BATCH_SIZE)):\n",
    "        batch = torch.LongTensor(padded[BATCH_SIZE*i:BATCH_SIZE*(i+1)]) \n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[BATCH_SIZE*i:BATCH_SIZE*(i+1)])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].numpy())\n",
    "        \n",
    "    #полученные эмбеддинги будут признаками для обучения модели\n",
    "    bert_features = np.concatenate(embeddings) \n",
    "    print('shape:', bert_features.shape)\n",
    "    return bert_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e3cc9d41cc4f1dac65ebc0aab8d2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "shape: (200, 768)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_bert_train = bert(train_bert['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b461727a9fa54a83b788783f2a67cd9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "shape: (200, 768)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_bert_test = bert(test_bert['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование  DistilBert моделью LogisticRegression<a id=\"2.3.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best scrores GridSearch:  0.5703703703703703\n",
      "best scrores:  {'C': 8.0, 'max_iter': 100.0}\n",
      "f1_score test: 0.7111111111111111\n"
     ]
    }
   ],
   "source": [
    "\n",
    "estimator = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "param_grid = {\n",
    "    'C': np.linspace(1, 8, 4), \n",
    "    'max_iter':np.linspace(100, 200, 3)\n",
    "}\n",
    "\n",
    "f1_score_model(searchCV(estimator, param_grid, X_bert_train, train_bert['toxic']),X_bert_train, train_bert['toxic'], X_bert_test, test_bert['toxic']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка на адекватность<a id=\"2.3.4\"></a>\n",
    "Насколько хороша метроика, мы можем узнать сравнив с результатами фиктивного классификатора, который покажет среднее распределение в целевом признаке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best scrores GridSearch:  0.09746261602784644\n",
      "best scrores:  {}\n",
      "f1_score test: 0.10058309037900875\n"
     ]
    }
   ],
   "source": [
    "estimator = DummyClassifier()\n",
    "param_grid = {}\n",
    "\n",
    "f1_score_model(searchCV(estimator, param_grid, vector_train, y_train)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат модели LogisticRegression f1 > 75 лучше чем DummyClassifier f1 = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наилучшие результаты задачи бинарной классификации текста показала модель логистической регрессии. Также предобученная модель bert может дать результаты. Для этой модели имеется возможность дообучить последний слой на нашем корпусе для более лучших показателей. RandomForestClassifier и CatBoostClassifier в этой задаче были безуспешны."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
