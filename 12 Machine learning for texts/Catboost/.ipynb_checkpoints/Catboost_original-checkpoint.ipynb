{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Текстовые функции и вложения в CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Установите GPU в качестве аппаратного ускорителя**\n",
    "\n",
    "    Прежде всего, вам нужно выбрать GPU в качестве аппаратного ускорителя. Для этого есть два простых шага:\n",
    "    Шаг 1. Перейдите к **Runtime** меню и выберите пункт **Change runtime type**\n",
    "    Шаг 2. Выбирать **GPU** в качестве аппаратного ускорителя.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "import catboost\n",
    "print(catboost.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом уроке мы будем использовать dataset IMDB из [Kaggle](https://www.kaggle.com) конкуренция за наши эксперименты. Данные могут быть загружены [here](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  One of the other reviewers has mentioned that ...      1\n",
       "1  A wonderful little production. <br /><br />The...      1\n",
       "2  I thought this was a wonderful way to spend ti...      1\n",
       "3  Basically there's a family where a little boy ...      0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget https://transfersh.com/ou7jB/imdb.csv -O imdb.csv\n",
    "df = pd.read_csv('imdb.csv')\n",
    "df['label'] = (df['sentiment'] == 'positive').astype(int)\n",
    "df.drop(['sentiment'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (40000, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, train_size=0.8, random_state=0)\n",
    "y_train, X_train = train_df['label'], train_df.drop(['label'], axis=1)\n",
    "y_test, X_test = test_df['label'], test_df.drop(['label'], axis=1)\n",
    "\n",
    "train_pool = Pool(data=X_train, label=y_train, text_features=['review'])\n",
    "test_pool = Pool(data=X_test, label=y_test, text_features=['review'])\n",
    "\n",
    "print('Train dataset shape: {}\\n'.format(train_pool.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9032955\ttest: 0.9093337\tbest: 0.9093337 (0)\ttotal: 282ms\tremaining: 4m 41s\n",
      "100:\tlearn: 0.9438666\ttest: 0.9402395\tbest: 0.9402395 (100)\ttotal: 31s\tremaining: 4m 35s\n",
      "200:\tlearn: 0.9534619\ttest: 0.9472547\tbest: 0.9472547 (200)\ttotal: 1m\tremaining: 4m 1s\n",
      "300:\tlearn: 0.9592404\ttest: 0.9506824\tbest: 0.9506824 (300)\ttotal: 1m 30s\tremaining: 3m 30s\n",
      "400:\tlearn: 0.9635656\ttest: 0.9529911\tbest: 0.9529911 (400)\ttotal: 1m 57s\tremaining: 2m 55s\n",
      "500:\tlearn: 0.9668989\ttest: 0.9545205\tbest: 0.9545205 (500)\ttotal: 2m 23s\tremaining: 2m 22s\n",
      "600:\tlearn: 0.9697463\ttest: 0.9555450\tbest: 0.9555450 (600)\ttotal: 2m 48s\tremaining: 1m 51s\n",
      "700:\tlearn: 0.9720714\ttest: 0.9563217\tbest: 0.9563217 (700)\ttotal: 3m 13s\tremaining: 1m 22s\n",
      "800:\tlearn: 0.9740076\ttest: 0.9569512\tbest: 0.9569522 (799)\ttotal: 3m 37s\tremaining: 54.1s\n",
      "900:\tlearn: 0.9751890\ttest: 0.9572400\tbest: 0.9572472 (897)\ttotal: 3m 56s\tremaining: 26s\n",
      "999:\tlearn: 0.9768668\ttest: 0.9576305\tbest: 0.9576390 (997)\ttotal: 4m 20s\tremaining: 0us\n",
      "bestTest = 0.9576390386\n",
      "bestIteration = 997\n",
      "Shrink model to first 998 iterations.\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "def fit_model(train_pool, test_pool, **kwargs):\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=1000,\n",
    "        learning_rate=0.05,\n",
    "        eval_metric='AUC',\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return model.fit(\n",
    "        train_pool,\n",
    "        eval_set=test_pool,\n",
    "        verbose=100\n",
    "    )\n",
    "\n",
    "model = fit_model(train_pool, test_pool, task_type='GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как это работает\n",
    "\n",
    "    1. Токенизация Текста\n",
    "    2. Создание Словаря\n",
    "    3. Расчет Характеристик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация Текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно мы получаем наш текст в виде последовательности символов Юникода. Таким образом, если задача не является классификацией ДНК, нам не нужна такая детализация, более того, нам нужно извлечь более сложные сущности, например слова. Процесс извлечения токенов-слов, цифр, знаков препинания или специальных символов, которые определяют эмодзи из последовательности, называется **токенизацией**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация-это первая часть предварительной обработки текста в CatBoost и выполняется как простое разбиение последовательности на строковый шаблон (например, пробел)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_small = [\n",
    "    \"Cats are so cute :)\",\n",
    "    \"Mouse scare...\",\n",
    "    \"The cat defeated the mouse\",\n",
    "    \"Cute: Mice gather an army!\",\n",
    "    \"Army of mice defeated the cat :(\",\n",
    "    \"Cat offers peace\",\n",
    "    \"Cat is scared :(\",\n",
    "    \"Cat and mouse live in peace :)\"\n",
    "]\n",
    "\n",
    "target_small = [1, 0, 1, 1, 0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Cats', 'are', 'so', 'cute', ':)'],\n",
       " ['Mouse', 'scare...'],\n",
       " ['The', 'cat', 'defeated', 'the', 'mouse'],\n",
       " ['Cute:', 'Mice', 'gather', 'an', 'army!'],\n",
       " ['Army', 'of', 'mice', 'defeated', 'the', 'cat', ':('],\n",
       " ['Cat', 'offers', 'peace'],\n",
       " ['Cat', 'is', 'scared', ':('],\n",
       " ['Cat', 'and', 'mouse', 'live', 'in', 'peace', ':)']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost.text_processing import Tokenizer\n",
    "\n",
    "simple_tokenizer = Tokenizer()\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    return [simple_tokenizer.tokenize(text) for text in texts]\n",
    "\n",
    "simple_tokenized_text = tokenize_texts(text_small)\n",
    "simple_tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительная предварительная обработка\n",
    "\n",
    "Давайте подробнее рассмотрим результат токенизации небольшого текстового примера-токены содержат много ошибок:\n",
    "\n",
    "1. Они склеены пунктуацией 'Cute:', 'army!', 'skare...'.\n",
    "2. Слово 'Cat' and 'cat', 'Mice' и 'mice' кажется, они имеют одно и то же значение, Возможно, это должны быть одни и те же символы.\n",
    "3. Та же проблема и с токенами 'are'/'is' -- это флективные формы одного и того же знака 'be'.\n",
    "    \n",
    "    **Пунктуационная обработка** , **строчной**, и **лемматизация** процессы помогают преодолеть эти проблемы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка знаков препинания и строчные буквы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cats', 'are', 'so', 'cute'],\n",
       " ['mouse', 'scare'],\n",
       " ['the', 'cat', 'defeated', 'the', 'mouse'],\n",
       " ['cute', 'mice', 'gather', 'an', 'army'],\n",
       " ['army', 'of', 'mice', 'defeated', 'the', 'cat'],\n",
       " ['cat', 'offers', 'peace'],\n",
       " ['cat', 'is', 'scared'],\n",
       " ['cat', 'and', 'mouse', 'live', 'in', 'peace']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(\n",
    "    lowercasing=True,\n",
    "    separator_type='BySense',\n",
    "    token_types=['Word', 'Number']\n",
    ")\n",
    "\n",
    "tokenized_text = [tokenizer.tokenize(text) for text in text_small]\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление стоп-слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стоп - слова** -слова, которые считаются неинформативными в этой задаче, например функциональные слова, такие как, is, at, which, on. Обычно стоп-слова удаляются во время предварительной обработки текста, чтобы уменьшить объем информации, которая рассматривается для дальнейших алгоритмов. Стоп-слова собираются вручную (в виде словаря) или автоматически, например, беря наиболее частые слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cats', 'so', 'cute'],\n",
       " ['mouse', 'scare'],\n",
       " ['cat', 'defeated', 'mouse'],\n",
       " ['cute', 'mice', 'gather', 'army'],\n",
       " ['army', 'mice', 'defeated', 'cat'],\n",
       " ['cat', 'offers', 'peace'],\n",
       " ['cat', 'scared'],\n",
       " ['cat', 'mouse', 'live', 'peace']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(('be', 'is', 'are', 'the', 'an', 'of', 'and', 'in'))\n",
    "\n",
    "def filter_stop_words(tokens):\n",
    "    return list(filter(lambda x: x not in stop_words, tokens))\n",
    "    \n",
    "tokenized_text_no_stop = [filter_stop_words(tokens) for tokens in tokenized_text]\n",
    "tokenized_text_no_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемма (Википедия) - это каноническая форма, словарная форма или форма цитирования набора слов.\n",
    "Например, Лемма \"go\" представляет собой формы \"go\", \"goes\", \"going\", \"went\", and \"gone\". Процесс преобразования слова в его лемму называется **лемматизация**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\python_dev\\AppData\\Roaming\\Python\\Python37\\si\n",
      "[nltk_data]     te-packages\\nltk\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk_data_path = os.path.join(os.path.dirname(nltk.__file__), 'nltk_data')\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "nltk.download('wordnet', nltk_data_path)\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens_nltk(tokens):\n",
    "    return list(map(lambda t: lemmatizer.lemmatize(t), tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cat', 'so', 'cute'],\n",
       " ['mouse', 'scare'],\n",
       " ['cat', 'defeated', 'mouse'],\n",
       " ['cute', 'mouse', 'gather', 'army'],\n",
       " ['army', 'mouse', 'defeated', 'cat'],\n",
       " ['cat', 'offer', 'peace'],\n",
       " ['cat', 'scared'],\n",
       " ['cat', 'mouse', 'live', 'peace']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_small_lemmatized_nltk = [lemmatize_tokens_nltk(tokens) for tokens in tokenized_text_no_stop]\n",
    "text_small_lemmatized_nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь слова с одинаковым значением представлены одним и тем же маркером, лексемы не склеиваются с пунктуацией.\n",
    "\n",
    "Будьте осторожны. Вы должны проверить это для своей собственной задачи:\n",
    "Действительно ли необходимо удалять знаки препинания, строчные предложения или выполнять лемматизацию и/или токенизацию слов?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Давайте проверим точность с помощью новой предварительной обработки текста\n",
    "Поскольку CatBoost не выполняет интервальную пунктуацию, строчные буквы и лемматизацию, нам нужно предварительно обработать текст вручную, а затем передать его алгоритму обучения.\n",
    "\n",
    "Поскольку естественными текстовыми признаками являются только конспект и обзор, мы будем предварительно обрабатывать только их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 51.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def preprocess_data(X):\n",
    "    X_preprocessed = X.copy()\n",
    "    X_preprocessed['review'] = X['review'].apply(lambda x: ' '.join(lemmatize_tokens_nltk(tokenizer.tokenize(x))))\n",
    "    return X_preprocessed\n",
    "\n",
    "X_preprocessed_train = preprocess_data(X_train)\n",
    "X_preprocessed_test = preprocess_data(X_test)\n",
    "\n",
    "train_processed_pool = Pool(\n",
    "    X_preprocessed_train, y_train, \n",
    "    text_features=['review'],\n",
    ")\n",
    "\n",
    "test_processed_pool = Pool(\n",
    "    X_preprocessed_test, y_test, \n",
    "    text_features=['review'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9053034\ttest: 0.9086902\tbest: 0.9086902 (0)\ttotal: 280ms\tremaining: 4m 39s\n",
      "100:\tlearn: 0.9477155\ttest: 0.9435817\tbest: 0.9435817 (100)\ttotal: 30.3s\tremaining: 4m 29s\n",
      "200:\tlearn: 0.9581237\ttest: 0.9518494\tbest: 0.9518494 (200)\ttotal: 58.4s\tremaining: 3m 52s\n",
      "300:\tlearn: 0.9643533\ttest: 0.9563589\tbest: 0.9563589 (300)\ttotal: 1m 25s\tremaining: 3m 18s\n",
      "400:\tlearn: 0.9687793\ttest: 0.9592177\tbest: 0.9592177 (400)\ttotal: 1m 53s\tremaining: 2m 49s\n",
      "500:\tlearn: 0.9722002\ttest: 0.9609626\tbest: 0.9609626 (500)\ttotal: 2m 22s\tremaining: 2m 22s\n",
      "600:\tlearn: 0.9748282\ttest: 0.9621297\tbest: 0.9621297 (600)\ttotal: 2m 51s\tremaining: 1m 54s\n",
      "700:\tlearn: 0.9770209\ttest: 0.9628819\tbest: 0.9628819 (700)\ttotal: 3m 20s\tremaining: 1m 25s\n",
      "800:\tlearn: 0.9789707\ttest: 0.9635074\tbest: 0.9635074 (800)\ttotal: 3m 48s\tremaining: 56.7s\n",
      "900:\tlearn: 0.9805882\ttest: 0.9640339\tbest: 0.9640339 (900)\ttotal: 10m 29s\tremaining: 1m 9s\n",
      "999:\tlearn: 0.9819204\ttest: 0.9644448\tbest: 0.9644448 (999)\ttotal: 10m 53s\tremaining: 0us\n",
      "bestTest = 0.9644448161\n",
      "bestIteration = 999\n"
     ]
    }
   ],
   "source": [
    "model_on_processed_data = fit_model(train_processed_pool, test_processed_pool, task_type='GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9576390385627747 vs 0.9644448161125183 (+0.71%)\n"
     ]
    }
   ],
   "source": [
    "def print_score_diff(first_model, second_model):\n",
    "    first_accuracy = first_model.best_score_['validation']['AUC']\n",
    "    second_accuracy = second_model.best_score_['validation']['AUC']\n",
    "\n",
    "    gap = (second_accuracy - first_accuracy) / first_accuracy * 100\n",
    "\n",
    "    print('{} vs {} ({:+.2f}%)'.format(first_accuracy, second_accuracy, gap))\n",
    "    \n",
    "print_score_diff(model, model_on_processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание Словаря"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После первого этапа, предварительной обработки текста и токенизации, начинается второй этап. Второй этап использует подготовленный текст для выбора набора единиц измерения, которые будут использоваться для построения новых числовых признаков.\n",
    "\n",
    "Набор выбранных единиц называется словарем. Он может содержать слова, биграммы слов или символьные n-граммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost.text_processing import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cat', 'so', 'cute'],\n",
       " ['mouse', 'scare'],\n",
       " ['cat', 'defeated', 'mouse'],\n",
       " ['cute', 'mouse', 'gather', 'army'],\n",
       " ['army', 'mouse', 'defeated', 'cat'],\n",
       " ['cat', 'offer', 'peace'],\n",
       " ['cat', 'scared'],\n",
       " ['cat', 'mouse', 'live', 'peace']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_small_lemmatized_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(occurence_lower_bound=0, max_dictionary_size=10)\n",
    "\n",
    "dictionary.fit(text_small_lemmatized_nltk);\n",
    "#dictionary.fit(text_small, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"cat\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "dictionary.save('dictionary.tsv')\n",
    "!cat dictionary.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 3]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.apply([text_small_lemmatized_nltk[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Расчет Фитчей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразование в векторы фиксированного размера\n",
    "\n",
    "Большинство классических алгоритмов ML вычисляют и выполняют предсказания на фиксированном числе объектов $F$.<br>\n",
    "Это означает, что набор обучения $X= (x_i) $ содержит векторы $x_i = (a_0, a_1, ..., a_F)$ где  $F$ константа.    \n",
    "\n",
    "Так как текстовый объект $x$ это не вектор фиксированной длины, нам нужно выполнить предварительную обработку исходного набора $D$.<br>\n",
    "Одним из самых простых методов кодирования текста в вектор является **Мешок слов (BoW)**.\n",
    "\n",
    "### Алгоритм мешка слов\n",
    "\n",
    "Алгоритм принимает в себя словарь и текст.<br>\n",
    "Во время работы алгоритма текст $x = (a_0, a_1, ..., a_k)$ преобразовано в вектор $\\\\tilde x = (b_0, b_1, ..., b_F)$,<br> где  $b_i$ это 0/1 (в зависимости от того, есть ли слово с id=$i$ из словаря в текст $x$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_proc_train_small, y_train_small = X_preprocessed_train[:1000]['review'].to_list(), y_train[:1000]\n",
    "X_proc_train_small = list(map(simple_tokenizer.tokenize, X_proc_train_small))\n",
    "X_proc_test_small, y_test_small = X_preprocessed_test[:1000]['review'].to_list(), y_test[:1000]\n",
    "X_proc_test_small = list(map(simple_tokenizer.tokenize, X_proc_test_small))\n",
    "\n",
    "dictionary = Dictionary(max_dictionary_size=100)\n",
    "dictionary.fit(X_proc_train_small);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bag_of_words(tokenized_text, dictionary):\n",
    "    features = np.zeros((len(tokenized_text), dictionary.size))\n",
    "    for i, tokenized_sentence in enumerate(tokenized_text):\n",
    "        indices = np.array(dictionary.apply([tokenized_sentence])[0])\n",
    "        if len(indices) > 0:\n",
    "            features[i, indices] = 1\n",
    "    return features\n",
    "\n",
    "X_bow_train_small = bag_of_words(X_proc_train_small, dictionary)\n",
    "X_bow_test_small = bag_of_words(X_proc_test_small, dictionary)\n",
    "X_bow_train_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def fit_linear_model(X, y):\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def evaluate_model_auc(model, X, y):\n",
    "    y_pred = model.predict_proba(X)[:,1]\n",
    "    metric = roc_auc_score(y, y_pred)\n",
    "    print('AUC: ' + str(metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model\n",
      "AUC: 0.7158704813129555\n",
      "Comparing to constant prediction\n",
      "AUC: 0.5\n"
     ]
    }
   ],
   "source": [
    "def evaluate_models(X_train, y_train, X_test, y_test):\n",
    "    linear_model = fit_linear_model(X_train, y_train)\n",
    "        \n",
    "    print('Linear model')\n",
    "    evaluate_model_auc(linear_model, X_test, y_test)\n",
    "    print('Comparing to constant prediction')\n",
    "    auc_constant_prediction = roc_auc_score(y_test, np.ones(shape=(len(y_test), 1)) * 0.5)\n",
    "    print('AUC: ' + str(auc_constant_prediction))\n",
    "    \n",
    "evaluate_models(X_bow_train_small, y_train_small, X_bow_test_small, y_test_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1000)\n",
      "Linear model\n",
      "AUC: 0.8715258184961521\n",
      "Comparing to constant prediction\n",
      "AUC: 0.5\n"
     ]
    }
   ],
   "source": [
    "unigram_dictionary = Dictionary(occurence_lower_bound=0, max_dictionary_size=1000)\n",
    "unigram_dictionary.fit(X_proc_train_small)\n",
    "\n",
    "X_bow_train_small = bag_of_words(X_proc_train_small, unigram_dictionary)\n",
    "X_bow_test_small = bag_of_words(X_proc_test_small, unigram_dictionary)\n",
    "print(X_bow_train_small.shape)\n",
    "\n",
    "evaluate_models(X_bow_train_small, y_train_small, X_bow_test_small, y_test_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Глядя на последовательности букв / слов\n",
    "\n",
    "Давайте рассмотрим пример: тексты \"кошка победила мышь\" и \" армия мышей победила кошку:('<br>\n",
    "Упрощая его, мы имеем три лексемы в каждом предложении \"кошка побеждает мышь\" и \"мышь побеждает кошку'.<br>\n",
    "После применения лука мы получаем два равных вектора с противоположным значением:\n",
    "\n",
    "| cat | mouse | defeat |\n",
    "|-----|-------|--------|\n",
    "| 1   | 1     | 1      |\n",
    "| 1   | 1     | 1      |\n",
    "\n",
    "Как их отличить?\n",
    "Давайте добавим последовательности слов в виде отдельных лексем в наш словарь:\n",
    "\n",
    "| cat | mouse | defeat | cat_defeat | mouse_defeat | defeat_cat | defeat_mouse |\n",
    "|-----|-------|--------|------------|--------------|------------|--------------|\n",
    "| 1   | 1     | 1      | 1          | 0            | 0          | 1            |\n",
    "| 1   | 1     | 1      | 0          | 1            | 1          | 0            |\n",
    "\n",
    "**N-gram** это непрерывная последовательность $n$ элементов из заданного образца текста или речи (Wikipedia).<br>\n",
    "В приведенном выше примере Bi-gram (Bigram)  = 2 слова\n",
    "\n",
    "Nграммы помогают добавить в векторы больше информации о структуре текста, более того, существуют n-граммы, не имеющие значения в разделении, например, 'Mickey Mouse company'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_catboost.Dictionary at 0x2896d1b0b70>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = Dictionary(occurence_lower_bound=0, gram_order=2)\n",
    "dictionary.fit(text_small_lemmatized_nltk)\n",
    "\n",
    "dictionary.save('dictionary.tsv')\n",
    "# !cat dictionary.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5000)\n",
      "Linear model\n",
      "AUC: 0.8281568279047059\n",
      "Comparing to constant prediction\n",
      "AUC: 0.5\n"
     ]
    }
   ],
   "source": [
    "bigram_dictionary = Dictionary(occurence_lower_bound=0, max_dictionary_size=5000, gram_order=2)\n",
    "bigram_dictionary.fit(X_proc_train_small)\n",
    "\n",
    "X_bow_train_small = bag_of_words(X_proc_train_small, bigram_dictionary)\n",
    "X_bow_test_small = bag_of_words(X_proc_test_small, bigram_dictionary)\n",
    "print(X_bow_train_small.shape)\n",
    "\n",
    "evaluate_models(X_bow_train_small, y_train_small, X_bow_test_small, y_test_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram + Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 6000)\n",
      "Linear model\n",
      "AUC: 0.8916235457961653\n",
      "Comparing to constant prediction\n",
      "AUC: 0.5\n"
     ]
    }
   ],
   "source": [
    "X_bow_train_small = np.concatenate((\n",
    "    bag_of_words(X_proc_train_small, unigram_dictionary),\n",
    "    bag_of_words(X_proc_train_small, bigram_dictionary)\n",
    "), axis=1)\n",
    "X_bow_test_small = np.concatenate((\n",
    "    bag_of_words(X_proc_test_small, unigram_dictionary),\n",
    "    bag_of_words(X_proc_test_small, bigram_dictionary)\n",
    "), axis=1)\n",
    "print(X_bow_train_small.shape)\n",
    "\n",
    "evaluate_models(X_bow_train_small, y_train_small, X_bow_test_small, y_test_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имя параметра:\n",
    "\n",
    "1. **Text Tokenization** - `tokenizers`\n",
    "2. **Dictionary Creation** - `dictionaries`\n",
    "3. **Feature Calculation** - `feature_calcers`\n",
    "\n",
    "\\* Более сложная конфигурация с `text_processing` параметр"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tokenizers`\n",
    "\n",
    "Tokenizers used to preprocess Text type feature columns before creating the dictionary.\n",
    "\n",
    "[Documentation](https://catboost.ai/docs/references/tokenizer_options.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = [{\n",
    "    'tokenizerId': 'Space',\n",
    "    'delimiter': ' ',\n",
    "    'separator_type': 'ByDelimiter',\n",
    "},{\n",
    "    'tokenizerId': 'Sense',\n",
    "    'separator_type': 'BySense',\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dictionaries`\n",
    "\n",
    "Dictionaries used to preprocess Text type feature columns.\n",
    "\n",
    "[Documentation](https://catboost.ai/docs/references/dictionaries_options.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionaries = [{\n",
    "    'dictionaryId': 'Unigram',\n",
    "    'max_dictionary_size': '50000',\n",
    "    'gram_count': '1',\n",
    "},{\n",
    "    'dictionaryId': 'Bigram',\n",
    "    'max_dictionary_size': '50000',\n",
    "    'gram_count': '2',\n",
    "},{\n",
    "    'dictionaryId': 'Trigram',\n",
    "    'token_level_type': 'Letter',\n",
    "    'max_dictionary_size': '50000',\n",
    "    'gram_count': '3',\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `feature_calcers`\n",
    "\n",
    "Калькуляторы объектов используются для расчета новых объектов на основе предварительно обработанных столбцов объектов текстового типа.\n",
    "\n",
    "1. **`BoW`**<br>\n",
    "Мешок слов: 0/1 features (образец текста имеет или не имеет token_id).<br>\n",
    "Количество произведенных числовые характеристики = размер словаря.<br>\n",
    "параметр: `top_tokens_count` - максимальное количество токенов, которые будут использоваться для векторизации в мешке слов, наиболее частые $n$ жетоны принимаются (**сильно влияет как на использование CPU ang GPU RAM**).\n",
    "\n",
    "2. **`NaiveBayes`**<br>\n",
    "NaiveBayes: [Полиномиальное упрощенного алгоритма Байеса](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Multinomial_naive_Bayes) модель. Добавлено столько же новых функций, сколько и классов. Эта функция вычисляется по аналогии со счетчиками в CatBoost путем перестановки ([оценка и показатели CTR](https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html)). Другими словами, производится случайная перестановка, а затем мы идем сверху вниз по набору данных и вычисляем вероятность его принадлежности к этому классу для каждого объекта.\n",
    "\n",
    "3. **`BM25`**<br>\n",
    "[BM25](https://en.wikipedia.org/wiki/Okapi_BM25). Добавлено столько же новых функций, сколько и классов. Идея та же, что и в наивном Байесе, но для каждого класса мы вычисляем не условную вероятность, а определенную релевантность, что аналогично tf-idf, где лексемы вместо слов и классы вместо документов (точнее, объединение всех текстов этого класса). Только множитель tf в BM25 заменяется другим множителем, что дает преимущество классам, содержащим редкие токены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_calcers = [\n",
    "    'BoW:top_tokens_count=1000',\n",
    "    'NaiveBayes',\n",
    "    'BM25',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `text_processing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processing = {\n",
    "    \"tokenizers\" : [{\n",
    "        \"tokenizer_id\" : \"Space\",\n",
    "        \"separator_type\" : \"ByDelimiter\",\n",
    "        \"delimiter\" : \" \"\n",
    "    }],\n",
    "\n",
    "    \"dictionaries\" : [{\n",
    "        \"dictionary_id\" : \"BiGram\",\n",
    "        \"max_dictionary_size\" : \"50000\",\n",
    "        \"occurrence_lower_bound\" : \"3\",\n",
    "        \"gram_order\" : \"2\"\n",
    "    }, {\n",
    "        \"dictionary_id\" : \"Word\",\n",
    "        \"max_dictionary_size\" : \"50000\",\n",
    "        \"occurrence_lower_bound\" : \"3\",\n",
    "        \"gram_order\" : \"1\"\n",
    "    }],\n",
    "\n",
    "    \"feature_processing\" : {\n",
    "        \"default\" : [{\n",
    "            \"dictionaries_names\" : [\"BiGram\", \"Word\"],\n",
    "            \"feature_calcers\" : [\"BoW\"],\n",
    "            \"tokenizers_names\" : [\"Space\"]\n",
    "        }, {\n",
    "            \"dictionaries_names\" : [\"Word\"],\n",
    "            \"feature_calcers\" : [\"NaiveBayes\"],\n",
    "            \"tokenizers_names\" : [\"Space\"]\n",
    "        }],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Резюме: текстовые функции в CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритм:\n",
    "1. Входной текст загружается в виде обычного столбца. ``text_column: [string]``.\n",
    "2. Каждый образец текста маркируется с помощью разбиения на пробелы. ``tokenized_column: [[string]]``.\n",
    "3. Оценка словаря.\n",
    "4. Каждая строка в маркированном столбце преобразуется в token_id из словаря. ``text: [[token_id]]``.\n",
    "5. В зависимости от параметров CatBoost производит функции на основе результирующего текстового столбца: Bag of words, Multinomial naive bayes или Bm25.\n",
    "6. Вычисленные объекты float передаются в обычный алгоритм обучения CatBoost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings In CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получить Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n",
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "#big_model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')\n",
    "#X_embed_train = big_model.encode(X_train['review'].to_list())\n",
    "#X_embed_test = big_model.encode(X_test['review'].to_list())\n",
    "\n",
    "!wget https://transfersh.com/HDHxy/embedded_train.npy -O embedded_train.npy\n",
    "X_embed_train = np.load('embedded_train.npy')\n",
    "\n",
    "!wget https://transfersh.com/whOm3/embedded_test.npy -O embedded_test.npy\n",
    "X_embed_test = np.load('embedded_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embed_first_train_small, y_first_train_small = X_embed_train[:5000], y_train[:5000]\n",
    "X_embed_second_train_small, y_second_train_small = X_embed_train[5000:10000], y_train[5000:10000]\n",
    "X_embed_test_small, y_test_small = X_embed_test[:5000], y_test[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Чистые embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model\n",
      "AUC: 0.893788744413289\n",
      "Comparing to constant prediction\n",
      "AUC: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files (x86)\\microsoft visual studio\\shared\\python37_64\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "evaluate_models(X_embed_second_train_small, y_second_train_small, X_embed_test_small, y_test_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### линейный дискриминантный анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1)\n",
      "Linear model\n",
      "AUC: 0.9076231221360127\n",
      "Comparing to constant prediction\n",
      "AUC: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis(solver='svd')\n",
    "lda.fit(X_embed_first_train_small, y_first_train_small)\n",
    "\n",
    "X_lda_train_small = lda.transform(X_embed_second_train_small)\n",
    "X_lda_test_small = lda.transform(X_embed_test_small)\n",
    "print(X_lda_train_small.shape)\n",
    "evaluate_models(X_lda_train_small, y_second_train_small, X_lda_test_small, y_test_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings in CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('train_embed_text.tsv', 'w', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t', quotechar='\"')\n",
    "    for y, text, row in zip(y_train, X_preprocessed_train['review'].to_list(), X_embed_train):\n",
    "        writer.writerow((str(y), text, ';'.join(map(str, row))))\n",
    "\n",
    "with open('test_embed_text.tsv', 'w', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t', quotechar='\"')\n",
    "    for y, text, row in zip(y_test, X_preprocessed_test['review'].to_list(), X_embed_test):\n",
    "        writer.writerow((str(y), text, ';'.join(map(str, row))))\n",
    "        \n",
    "with open('pool_text.cd', 'w', encoding='utf-8') as f:\n",
    "    f.write(\n",
    "        '0\\tLabel\\n'\\\n",
    "        '1\\tText\\n'\\\n",
    "        '2\\tNumVector'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "CatBoostError",
     "evalue": "Error in dsv data. Line 1: Column 2 (type NumVector, value = \"-0.9223569;1.5910692;-0.22656153;-0.5205022;0.020115484;-0.28878975;-1.6100531;0.12747245;-0.48439464;0.9254346;0.11817273;-0.27499163;0.91859883;0.029993558;-0.41534898;-1.195413;0.48914075;-0.7213514;0.8003551;-0.15881045;-0.84909546;-0.36135992;0.65373003;0.21347566;0.6962879;0.10132878;1.0888671;1.6472787;0.429725;-0.13144816;0.26708478;-0.14857201;-1.0308561;-0.58111495;0.06597881;-0.043940995;-0.75461006;-0.7918675;-0.08388872;0.5921703;-0.25132263;-0.43747708;1.3906786;1.5870262;-0.7198363;1.8372939;0.4360829;0.59700537;0.76579326;-0.36482832;-1.1962535;0.1079734;-1.3399609;0.25571755;0.7992953;-1.0416442;0.53340197;0.03728838;-0.0013935412;0.46585634;-0.48413163;1.1476865;0.28319475;-1.2835209;0.18342014;0.9869629;0.31597224;0.18192711;0.81881464;0.06738139;-0.034248184;1.7800509;-0.6446338;1.1177249;0.25750065;0.53501153;-0.53949034;-0.57355046;-0.19496007;1.1623977;-0.34042096;0.34276825;-0.48455554;-0.7285781;-1.2059894;-0.65008664;-0.14688623;-0.27770743;-0.92768145;-0.80582535;2.42885;-0.011778862;-0.15547118;0.17173201;0.43481648;-0.16670349;0.83995056;-1.4491017;-0.96386826;-0.13784541;0.30878335;1.4850018;-0.128756;1.8382106;0.6827408;-0.4977963;-1.0919545;-0.80772096;0.9417971;0.033919062;-1.5233915;1.2777625;-1.5763589;-0.31212112;0.078078896;-0.80070734;-0.42357063;0.54369926;-1.1209228;-0.2593717;-0.36159822;-1.9484012;-1.018327;0.51438993;0.34731716;0.033662267;-1.8727475;0.7552428;0.46538076;-1.4290526;0.6562162;-1.6337106;0.41917384;0.3644207;-0.65158993;1.9418066;-0.05977349;-0.41855007;-0.96712965;-1.2750119;0.12125729;1.0410511;0.5127865;1.6733811;0.8196238;0.28702554;0.37327373;0.12492058;0.46755522;1.4473456;0.10592704;0.7800591;0.6762344;0.7775878;-1.7473623;0.5290827;-0.4564283;0.9212256;1.016764;-0.12657486;-1.6181562;1.0460216;-0.57367134;1.1559446;1.7166809;-0.15706097;-0.91218096;0.22942396;-0.19145176;1.3955494;1.042763;1.1985916;1.256142;0.1090519;1.6293585;0.12618642;-0.4646231;-0.20878729;0.61328065;1.1167374;-0.87862015;0.63010645;-0.9693043;-0.4535649;-0.28663298;0.47177625;-0.059245344;0.6876031;-0.31758058;0.95172817;0.08782523;-0.058655016;0.43434682;-1.6029655;-0.883187;0.012064548;0.9030134;-1.5629709;-0.8497113;0.04049898;0.1743534;-0.1311008;1.0266913;0.26701522;-0.59673643;1.3085759;-0.21993507;0.7294843;-0.4757308;2.0802963;0.42047563;1.613442;-0.528652;-0.13180348;0.45596397;0.825602;-0.19632655;0.4972855;0.58982706;0.13822094;1.7208109;1.2458025;0.54768705;1.3640258;-0.7864491;0.52970874;1.1898146;-0.8484105;0.5599441;-0.21746774;-0.014687417;-1.3609505;0.7889931;-0.3836822;-0.56348276;0.5584196;0.5037796;-1.1330106;-0.66330606;-0.28818676;-2.1721342;-0.4160718;-0.20377265;-0.7731093;0.59211093;-1.5266719;-1.5022944;-0.5815167;0.4391628;1.2914863;0.13734934;-0.0026418895;0.63511646;1.2698038;1.2028072;-0.66317314;0.037537035;1.1181607;-0.22567281;0.603001;-0.1571307;0.6179716;1.029653;-0.4447545;0.357933;1.9192574;-0.95680654;0.116916314;-0.10980177;-0.1445123;0.47588322;0.56476355;0.7476216;-1.266278;-1.6116204;0.32409123;0.28907716;1.1755179;0.44870904;1.0280567;0.6399897;0.59790605;-0.032570723;0.39902267;0.62921053;0.24675933;0.5278491;0.18525541;0.52102923;0.097536005;-0.50816494;-2.2120757;0.59371;-0.9436819;-1.490031;-0.46870065;0.22552297;-0.15156937;1.4463185;0.34517482;-0.08388973;0.059815332;-0.8806351;1.0994834;0.34737232;1.0200876;0.41274172;-0.19930677;0.5385565;1.2933643;0.20846456;-0.6521027;0.30383533;0.28404754;-0.27834955;1.0791788;-0.46214545;0.20749898;0.57696575;-0.29764166;0.4838205;1.014835;-0.2887944;-0.4908947;-0.5333797;0.15787059;-1.0846032;-0.08522458;0.7442248;-1.5684034;0.9407593;-1.996068;-0.84386617;-1.2806346;0.9358251;-0.1317212;1.782782;0.7636822;-0.1070674;-1.1225357;-0.16882975;1.2650554;0.36299694;-0.087338425;-0.5819408;1.0218987;-1.1118195;0.25544673;0.098248824;1.3163892;0.21141456;-1.8226349;0.1396802;0.61421907;-1.8550094;1.047763;-0.57557744;0.2579147;-0.8209698;-0.32801086;1.079026;0.7670008;-1.064316;-0.36079314;0.2394009;0.70297915;0.17146568;-1.2863793;-0.6788032;-1.6295587;0.08722933;1.0865157;-0.97441024;-1.0282178;0.2414544;0.15090263;-1.6535442;-0.48938277;-1.6669629;0.80412173;0.08572138;0.59940284;0.21920891;0.7519167;-0.96461093;-0.13315433;1.0961684;-1.0273516;0.52350634;0.21509188;-2.5777364;-0.24063084;0.27052084;1.281958;0.29539746;-0.77566284;0.24520855;-1.2206914;-1.4486232;1.0374155;1.593905;-0.052590705;-0.41626108;0.12879558;0.20572959;0.9721003;0.07963175;-0.2560666;-0.43469915;0.91294426;0.37924457;0.26848882;0.17656438;-0.30905595;0.06976077;0.5854028;-1.723155;-0.36372373;0.8387147;0.7855268;0.8742433;-1.3853492;-0.0011953865;0.083482824;0.3008344;-1.5190121;-0.09642689;0.5169448;0.018470565;0.5372383;-0.8899572;1.1560798;0.22568125;0.34771878;0.48389876;-1.078133;1.7576027;-0.22320616;-0.720022;0.114717305;-0.9060612;-0.27281213;0.24383369;-0.96903807;-0.75262094;1.698879;1.2743233;-0.10166467;0.12914936;0.7340122;2.3941402;-0.029899897;-0.90146565;-0.30280527;-0.612486;0.24039975;-1.0499762;-0.24532685;0.48227593;-0.8247444;-0.63605434;1.1428456;0.35045156;-0.39231473;-0.69013923;1.0392905;0.017798906;-1.2462156;0.8500951;-0.2191514;-0.049079847;0.6510523;-0.8584391;0.77490747;0.20618045;1.1763042;0.39588797;-1.5647504;-1.1951814;1.3056713;-0.6925259;-0.6139799;1.5075094;0.24032478;0.3939445;0.3873489;0.7642654;0.13015139;-0.52966046;-0.20083512;-1.3504913;0.32525924;-1.4624784;0.9993947;0.7649133;-0.9873147;1.3974261;-1.6149198;-0.131789;-0.70286787;-0.7345543;-0.05441395;0.7753426;1.5024198;0.33658594;-1.0375904;-0.39494067;-0.05036429;-1.0416325;-2.1329446;-0.082876794;-0.6266631;-0.29325366;-0.45941833;0.42751288;0.13214517;-1.9475796;0.23329452;0.9865741;0.38192207;-0.9477965;-0.83919525;0.81552905;-0.54509777;0.60673964;0.9401908;1.0574737;1.5201348;-1.4642174;-0.3076429;0.704376;-0.95873207;0.5033583;-0.39724067;-1.7050936;0.10054039;-0.17731403;0.06325707;-0.81026626;0.6228622;-1.3346434;1.3800954;-0.564507;-0.23086467;-0.49246597;-0.541554;-1.6114091;1.2100096;0.7223392;-0.4589171;0.44922158;0.1897606;0.08643484;0.114062525;0.2317759;-1.2528753;-0.32570845;0.99322087;-0.15735814;0.37687913;1.0656523;-0.44862887;-0.46105263;-0.64187515;-0.19385913;0.80002046;0.4457478;-1.0375278;-0.15073757;-0.27250552;-0.62887836;-1.5768651;-1.2002287;-1.4066008;1.2240542;1.579625;-1.3693408;1.341614;2.1677752;-0.3675807;-0.6857126;-1.9527643;1.0195776;0.088374995;1.13596;-1.1763531;-0.35309666;0.06394251;1.246416;-0.41159046;0.010598301;-1.083179;0.6400723;1.3398341;0.06803058;-1.3732721;-0.6180723;-0.46588603;0.44831392;0.10349061;0.34859282;-0.1826235;0.7155886;-0.79616463;0.04213908;-0.019851636;0.481711;0.52777237;1.2466644;2.09079;1.5152766;-2.6045837;-1.2005053;-0.12886053;-1.353282;-0.8858357;1.2051463;0.4085306;1.539931;0.0065999916;1.1151601;0.021426875;-0.47187075;-0.33191016;-1.1883812;0.85899734;-0.13003989;-0.34556854;0.818371;0.72485435;0.28964278;-1.6579738;0.76141053;1.1771096;-0.9007054;-0.41053542;0.10604495;-0.015399275;0.7021874;-1.0593286;-1.429985;-0.7594959;0.7511618;-0.16619287;-0.6313859;-1.7046387;1.3100396;-0.21128957;1.9942594;-1.6923057;-0.9621442;-0.8736367;-0.1243533;-1.7057724;-0.6359303;0.06400415;-0.19811668;1.8314459;-0.4308039;-0.8386083;0.64536285;-0.79806185;-0.184127;-1.0986267;0.1298925;0.6896562;-1.6947107;0.26580283;-0.42416546;0.65791345;-1.2326167;0.031607322;-0.7884851;0.83011156;0.778353;0.0056853457;0.19161268;0.12953866;0.79161924;-0.334782;-0.7501564;-0.18540576;-0.17342074;0.27745265;-0.53214127;-0.33429867;-0.42492104;0.09505946;0.18038657;0.32016572;-1.1644782;-0.9117879;0.5382676;0.6552217;0.45017627;-0.296459;-1.0965241;-0.38910833;0.4008442;0.32628796;0.71736765;-0.82914144;-0.29042003;1.3646737;-0.118043885;1.3333007;-0.8977927;0.11208972;-1.0425749;0.2538738;0.06565514;-0.61228615;0.07318721;0.054124545;0.053724136;-0.091574304;1.1122426;-0.6677881;-1.3366294;1.405332;-0.9500972;0.9551956;0.32554442;0.80191857;1.1346573;-0.08863502;0.5259086;-0.5435545;-0.16452408;1.207424;0.7745852;-0.1652403;-1.7056143;-0.5982946;0.8200409;-0.30255836;-2.035269;-1.7963483;-0.88950783;0.72469956;0.08733691;-1.2528226;-1.1824967;-0.6277839;-1.2740934;-0.54731286;-0.6933736;0.42871928;-0.0055444445;-1.2877862;0.29976037;-0.40131766;0.79535437;-0.36244148;0.84123766;-0.29183123;0.15738061;-0.18610322;-0.13585325;-1.8694327;-1.0609034;0.2643724;-0.07022198;-1.4331921;0.109166175;1.2054801;-1.0239406;-0.5059301;1.000952;-0.2174636;-0.67160106;-1.2087047;0.8514075;-0.19859992;0.69184184;-1.1888993;-0.6485151;-0.7543882;-0.4256566;-0.091923825;-1.1569918;-0.145585;-1.1534319;0.55316466;0.2557348;1.7570646;-1.5003899;-0.9274567;0.0046140365;0.7809983;0.80082875;0.24870935;1.7415925;1.0668985;-0.7904149;-0.6052914;0.3218388;0.11325637;-0.96457475;-0.47037202;0.23163198;0.95557743;0.81950325;-1.1162937;-0.2427375;-1.552687;-0.2693196;1.658825;-0.6582647;-1.4937768;1.3734194;-0.6394627;0.7295127;0.022157911;1.6094205;0.06776536;0.9045105;-0.23225495;0.48734573;0.2975744;-0.070913725;0.2988459;0.04456663;-0.7568035;-1.0622827;0.86060154;-0.2753061;0.12404157;0.42131668;0.40125248;1.2097604;1.3529545;-0.9450542;-0.40296143;0.050892234;-0.011720411;-2.9549088;0.5054355;0.05741853;-0.8282729;0.17578852;0.2918766;0.817105;-0.7136101;-0.7569359;0.3934715;-1.1290176;-0.26114485;-0.8671756;-0.6736791;-0.9816746;-0.57261103;-0.4977658;0.24745478;-0.049750954;0.057478543;0.5691232;0.45358962;-1.2356594;-0.23627892;0.038178716;0.62658507;-0.17901336;-0.37957004;0.20981042;-0.951273;-0.7293293;0.27312306;-0.8590248;0.6764361;0.557249;0.9328629;-0.04825418;0.58614033;-0.31715164;-0.30008665;-0.25409767;-0.27539137;1.267487;-0.7478815;1.3044462;-1.0350904;-0.29370117;1.2794406;1.5192382;9.964252e-06;0.37731764;-0.28013098;1.4869639;-0.24518044;0.7164149;0.22754243;-0.1777212;1.1375287;-0.7723231;0.93005335;-0.2276622;-0.053435925;0.5711656;0.3876687;-0.14534138;-0.26627508;-0.27550367;-0.67852837;0.030087916;0.65883934;-0.37487802;-0.7232355;-0.59450394;1.1781663;-0.8523654;0.59658104;-1.1875513;0.30816957;0.93801725;-0.62147367;-0.8252599;0.18190221;-1.0157545;-0.25232163;-0.29176566;-0.11901665;-0.4040245;0.06780063;-0.94582945;-0.8967268;0.7227656;0.36309245;-1.6479182;-0.11805678;0.32280815;-0.1471284;-0.4970696;-0.51049715;-0.37756032;-0.01693507;0.09975291;-0.46334785;-0.9150675;-0.71540093;-1.6445972;0.45055994;1.2120913;0.6598703;-0.5845387;-0.7248156;-0.3406537;-0.19192709;-1.3573699;0.06528028;-0.8333389;-0.41411433;0.49934512;-0.9470191;0.17148413;0.006261951;-0.8142561;0.8113596;-1.1511804;0.104508;-0.9359585;0.15516762;-0.33596322;-0.048122827;-1.5678782;-1.8427716;0.20602855;-0.44264787;-0.31032708;0.40390447;1.1531067;-1.1003097;-0.20466933;0.68588126;0.8000297;-0.9911266;-1.3094012;0.80726016;0.61110777;-1.4939072;-0.99671406;0.57437384;0.090929605;-1.2374773;0.736153;-2.1800125;-0.7902235;0.6748452;0.29221714;0.68703943;-0.1621678;-1.0730895;0.6031117;-1.58545;0.34596545;-1.8625246;-0.53073406;0.74101496;0.65389;0.3812936;-0.07169047;-0.97831345;0.5403425;0.4651145;-0.52683794;-1.2112619;-0.6166816;-0.10079314;1.3180608;0.4882683;0.44740295;-0.85653543;-0.107200816;-0.7712045;1.031362;-0.7955005;0.15063553;-0.3595464;-0.948605;1.6070462;1.3403572;-0.35564575;-1.7131515;1.9237846\r\"): c:/program files (x86)/go agent/pipelines/buildmaster/catboost.git/catboost/libs/data/cb_dsv_loader.cpp:139: Sub-field #1023 of numeric vector for feature 1 cannot be parsed as float. Check data contents or column description",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-9f0cc3c6e915>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_embed_pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_embed_text.tsv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pool_text.cd'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_embed_pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_embed_text.tsv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pool_text.cd'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, label, cat_features, text_features, embedding_features, column_description, pairs, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, feature_names, thread_count)\u001b[0m\n\u001b[0;32m    406\u001b[0m                         \u001b[1;34m\"feature_names should have None or string type when the pool is read from the file.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m                     )\n\u001b[1;32m--> 408\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhas_header\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_csv_quoting\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    409\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFeaturesData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(self, pool_file, column_description, pairs, feature_names_path, delimiter, has_header, ignore_csv_quoting, thread_count, quantization_params)\u001b[0m\n\u001b[0;32m    877\u001b[0m                 \u001b[0mignore_csv_quoting\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m                 \u001b[0mthread_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 879\u001b[1;33m                 \u001b[0mquantization_params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    880\u001b[0m             )\n\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._read_pool\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._read_pool\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCatBoostError\u001b[0m: Error in dsv data. Line 1: Column 2 (type NumVector, value = \"-0.9223569;1.5910692;-0.22656153;-0.5205022;0.020115484;-0.28878975;-1.6100531;0.12747245;-0.48439464;0.9254346;0.11817273;-0.27499163;0.91859883;0.029993558;-0.41534898;-1.195413;0.48914075;-0.7213514;0.8003551;-0.15881045;-0.84909546;-0.36135992;0.65373003;0.21347566;0.6962879;0.10132878;1.0888671;1.6472787;0.429725;-0.13144816;0.26708478;-0.14857201;-1.0308561;-0.58111495;0.06597881;-0.043940995;-0.75461006;-0.7918675;-0.08388872;0.5921703;-0.25132263;-0.43747708;1.3906786;1.5870262;-0.7198363;1.8372939;0.4360829;0.59700537;0.76579326;-0.36482832;-1.1962535;0.1079734;-1.3399609;0.25571755;0.7992953;-1.0416442;0.53340197;0.03728838;-0.0013935412;0.46585634;-0.48413163;1.1476865;0.28319475;-1.2835209;0.18342014;0.9869629;0.31597224;0.18192711;0.81881464;0.06738139;-0.034248184;1.7800509;-0.6446338;1.1177249;0.25750065;0.53501153;-0.53949034;-0.57355046;-0.19496007;1.1623977;-0.34042096;0.34276825;-0.48455554;-0.7285781;-1.2059894;-0.65008664;-0.14688623;-0.27770743;-0.92768145;-0.80582535;2.42885;-0.011778862;-0.15547118;0.17173201;0.43481648;-0.16670349;0.83995056;-1.4491017;-0.96386826;-0.13784541;0.30878335;1.4850018;-0.128756;1.8382106;0.6827408;-0.4977963;-1.0919545;-0.80772096;0.9417971;0.033919062;-1.5233915;1.2777625;-1.5763589;-0.31212112;0.078078896;-0.80070734;-0.42357063;0.54369926;-1.1209228;-0.2593717;-0.36159822;-1.9484012;-1.018327;0.51438993;0.34731716;0.033662267;-1.8727475;0.7552428;0.46538076;-1.4290526;0.6562162;-1.6337106;0.41917384;0.3644207;-0.65158993;1.9418066;-0.05977349;-0.41855007;-0.96712965;-1.2750119;0.12125729;1.0410511;0.5127865;1.6733811;0.8196238;0.28702554;0.37327373;0.12492058;0.46755522;1.4473456;0.10592704;0.7800591;0.6762344;0.7775878;-1.7473623;0.5290827;-0.4564283;0.9212256;1.016764;-0.12657486;-1.6181562;1.0460216;-0.57367134;1.1559446;1.7166809;-0.15706097;-0.91218096;0.22942396;-0.19145176;1.3955494;1.042763;1.1985916;1.256142;0.1090519;1.6293585;0.12618642;-0.4646231;-0.20878729;0.61328065;1.1167374;-0.87862015;0.63010645;-0.9693043;-0.4535649;-0.28663298;0.47177625;-0.059245344;0.6876031;-0.31758058;0.95172817;0.08782523;-0.058655016;0.43434682;-1.6029655;-0.883187;0.012064548;0.9030134;-1.5629709;-0.8497113;0.04049898;0.1743534;-0.1311008;1.0266913;0.26701522;-0.59673643;1.3085759;-0.21993507;0.7294843;-0.4757308;2.0802963;0.42047563;1.613442;-0.528652;-0.13180348;0.45596397;0.825602;-0.19632655;0.4972855;0.58982706;0.13822094;1.7208109;1.2458025;0.54768705;1.3640258;-0.7864491;0.52970874;1.1898146;-0.8484105;0.5599441;-0.21746774;-0.014687417;-1.3609505;0.7889931;-0.3836822;-0.56348276;0.5584196;0.5037796;-1.1330106;-0.66330606;-0.28818676;-2.1721342;-0.4160718;-0.20377265;-0.7731093;0.59211093;-1.5266719;-1.5022944;-0.5815167;0.4391628;1.2914863;0.13734934;-0.0026418895;0.63511646;1.2698038;1.2028072;-0.66317314;0.037537035;1.1181607;-0.22567281;0.603001;-0.1571307;0.6179716;1.029653;-0.4447545;0.357933;1.9192574;-0.95680654;0.116916314;-0.10980177;-0.1445123;0.47588322;0.56476355;0.7476216;-1.266278;-1.6116204;0.32409123;0.28907716;1.1755179;0.44870904;1.0280567;0.6399897;0.59790605;-0.032570723;0.39902267;0.62921053;0.24675933;0.5278491;0.18525541;0.52102923;0.097536005;-0.50816494;-2.2120757;0.59371;-0.9436819;-1.490031;-0.46870065;0.22552297;-0.15156937;1.4463185;0.34517482;-0.08388973;0.059815332;-0.8806351;1.0994834;0.34737232;1.0200876;0.41274172;-0.19930677;0.5385565;1.2933643;0.20846456;-0.6521027;0.30383533;0.28404754;-0.27834955;1.0791788;-0.46214545;0.20749898;0.57696575;-0.29764166;0.4838205;1.014835;-0.2887944;-0.4908947;-0.5333797;0.15787059;-1.0846032;-0.08522458;0.7442248;-1.5684034;0.9407593;-1.996068;-0.84386617;-1.2806346;0.9358251;-0.1317212;1.782782;0.7636822;-0.1070674;-1.1225357;-0.16882975;1.2650554;0.36299694;-0.087338425;-0.5819408;1.0218987;-1.1118195;0.25544673;0.098248824;1.3163892;0.21141456;-1.8226349;0.1396802;0.61421907;-1.8550094;1.047763;-0.57557744;0.2579147;-0.8209698;-0.32801086;1.079026;0.7670008;-1.064316;-0.36079314;0.2394009;0.70297915;0.17146568;-1.2863793;-0.6788032;-1.6295587;0.08722933;1.0865157;-0.97441024;-1.0282178;0.2414544;0.15090263;-1.6535442;-0.48938277;-1.6669629;0.80412173;0.08572138;0.59940284;0.21920891;0.7519167;-0.96461093;-0.13315433;1.0961684;-1.0273516;0.52350634;0.21509188;-2.5777364;-0.24063084;0.27052084;1.281958;0.29539746;-0.77566284;0.24520855;-1.2206914;-1.4486232;1.0374155;1.593905;-0.052590705;-0.41626108;0.12879558;0.20572959;0.9721003;0.07963175;-0.2560666;-0.43469915;0.91294426;0.37924457;0.26848882;0.17656438;-0.30905595;0.06976077;0.5854028;-1.723155;-0.36372373;0.8387147;0.7855268;0.8742433;-1.3853492;-0.0011953865;0.083482824;0.3008344;-1.5190121;-0.09642689;0.5169448;0.018470565;0.5372383;-0.8899572;1.1560798;0.22568125;0.34771878;0.48389876;-1.078133;1.7576027;-0.22320616;-0.720022;0.114717305;-0.9060612;-0.27281213;0.24383369;-0.96903807;-0.75262094;1.698879;1.2743233;-0.10166467;0.12914936;0.7340122;2.3941402;-0.029899897;-0.90146565;-0.30280527;-0.612486;0.24039975;-1.0499762;-0.24532685;0.48227593;-0.8247444;-0.63605434;1.1428456;0.35045156;-0.39231473;-0.69013923;1.0392905;0.017798906;-1.2462156;0.8500951;-0.2191514;-0.049079847;0.6510523;-0.8584391;0.77490747;0.20618045;1.1763042;0.39588797;-1.5647504;-1.1951814;1.3056713;-0.6925259;-0.6139799;1.5075094;0.24032478;0.3939445;0.3873489;0.7642654;0.13015139;-0.52966046;-0.20083512;-1.3504913;0.32525924;-1.4624784;0.9993947;0.7649133;-0.9873147;1.3974261;-1.6149198;-0.131789;-0.70286787;-0.7345543;-0.05441395;0.7753426;1.5024198;0.33658594;-1.0375904;-0.39494067;-0.05036429;-1.0416325;-2.1329446;-0.082876794;-0.6266631;-0.29325366;-0.45941833;0.42751288;0.13214517;-1.9475796;0.23329452;0.9865741;0.38192207;-0.9477965;-0.83919525;0.81552905;-0.54509777;0.60673964;0.9401908;1.0574737;1.5201348;-1.4642174;-0.3076429;0.704376;-0.95873207;0.5033583;-0.39724067;-1.7050936;0.10054039;-0.17731403;0.06325707;-0.81026626;0.6228622;-1.3346434;1.3800954;-0.564507;-0.23086467;-0.49246597;-0.541554;-1.6114091;1.2100096;0.7223392;-0.4589171;0.44922158;0.1897606;0.08643484;0.114062525;0.2317759;-1.2528753;-0.32570845;0.99322087;-0.15735814;0.37687913;1.0656523;-0.44862887;-0.46105263;-0.64187515;-0.19385913;0.80002046;0.4457478;-1.0375278;-0.15073757;-0.27250552;-0.62887836;-1.5768651;-1.2002287;-1.4066008;1.2240542;1.579625;-1.3693408;1.341614;2.1677752;-0.3675807;-0.6857126;-1.9527643;1.0195776;0.088374995;1.13596;-1.1763531;-0.35309666;0.06394251;1.246416;-0.41159046;0.010598301;-1.083179;0.6400723;1.3398341;0.06803058;-1.3732721;-0.6180723;-0.46588603;0.44831392;0.10349061;0.34859282;-0.1826235;0.7155886;-0.79616463;0.04213908;-0.019851636;0.481711;0.52777237;1.2466644;2.09079;1.5152766;-2.6045837;-1.2005053;-0.12886053;-1.353282;-0.8858357;1.2051463;0.4085306;1.539931;0.0065999916;1.1151601;0.021426875;-0.47187075;-0.33191016;-1.1883812;0.85899734;-0.13003989;-0.34556854;0.818371;0.72485435;0.28964278;-1.6579738;0.76141053;1.1771096;-0.9007054;-0.41053542;0.10604495;-0.015399275;0.7021874;-1.0593286;-1.429985;-0.7594959;0.7511618;-0.16619287;-0.6313859;-1.7046387;1.3100396;-0.21128957;1.9942594;-1.6923057;-0.9621442;-0.8736367;-0.1243533;-1.7057724;-0.6359303;0.06400415;-0.19811668;1.8314459;-0.4308039;-0.8386083;0.64536285;-0.79806185;-0.184127;-1.0986267;0.1298925;0.6896562;-1.6947107;0.26580283;-0.42416546;0.65791345;-1.2326167;0.031607322;-0.7884851;0.83011156;0.778353;0.0056853457;0.19161268;0.12953866;0.79161924;-0.334782;-0.7501564;-0.18540576;-0.17342074;0.27745265;-0.53214127;-0.33429867;-0.42492104;0.09505946;0.18038657;0.32016572;-1.1644782;-0.9117879;0.5382676;0.6552217;0.45017627;-0.296459;-1.0965241;-0.38910833;0.4008442;0.32628796;0.71736765;-0.82914144;-0.29042003;1.3646737;-0.118043885;1.3333007;-0.8977927;0.11208972;-1.0425749;0.2538738;0.06565514;-0.61228615;0.07318721;0.054124545;0.053724136;-0.091574304;1.1122426;-0.6677881;-1.3366294;1.405332;-0.9500972;0.9551956;0.32554442;0.80191857;1.1346573;-0.08863502;0.5259086;-0.5435545;-0.16452408;1.207424;0.7745852;-0.1652403;-1.7056143;-0.5982946;0.8200409;-0.30255836;-2.035269;-1.7963483;-0.88950783;0.72469956;0.08733691;-1.2528226;-1.1824967;-0.6277839;-1.2740934;-0.54731286;-0.6933736;0.42871928;-0.0055444445;-1.2877862;0.29976037;-0.40131766;0.79535437;-0.36244148;0.84123766;-0.29183123;0.15738061;-0.18610322;-0.13585325;-1.8694327;-1.0609034;0.2643724;-0.07022198;-1.4331921;0.109166175;1.2054801;-1.0239406;-0.5059301;1.000952;-0.2174636;-0.67160106;-1.2087047;0.8514075;-0.19859992;0.69184184;-1.1888993;-0.6485151;-0.7543882;-0.4256566;-0.091923825;-1.1569918;-0.145585;-1.1534319;0.55316466;0.2557348;1.7570646;-1.5003899;-0.9274567;0.0046140365;0.7809983;0.80082875;0.24870935;1.7415925;1.0668985;-0.7904149;-0.6052914;0.3218388;0.11325637;-0.96457475;-0.47037202;0.23163198;0.95557743;0.81950325;-1.1162937;-0.2427375;-1.552687;-0.2693196;1.658825;-0.6582647;-1.4937768;1.3734194;-0.6394627;0.7295127;0.022157911;1.6094205;0.06776536;0.9045105;-0.23225495;0.48734573;0.2975744;-0.070913725;0.2988459;0.04456663;-0.7568035;-1.0622827;0.86060154;-0.2753061;0.12404157;0.42131668;0.40125248;1.2097604;1.3529545;-0.9450542;-0.40296143;0.050892234;-0.011720411;-2.9549088;0.5054355;0.05741853;-0.8282729;0.17578852;0.2918766;0.817105;-0.7136101;-0.7569359;0.3934715;-1.1290176;-0.26114485;-0.8671756;-0.6736791;-0.9816746;-0.57261103;-0.4977658;0.24745478;-0.049750954;0.057478543;0.5691232;0.45358962;-1.2356594;-0.23627892;0.038178716;0.62658507;-0.17901336;-0.37957004;0.20981042;-0.951273;-0.7293293;0.27312306;-0.8590248;0.6764361;0.557249;0.9328629;-0.04825418;0.58614033;-0.31715164;-0.30008665;-0.25409767;-0.27539137;1.267487;-0.7478815;1.3044462;-1.0350904;-0.29370117;1.2794406;1.5192382;9.964252e-06;0.37731764;-0.28013098;1.4869639;-0.24518044;0.7164149;0.22754243;-0.1777212;1.1375287;-0.7723231;0.93005335;-0.2276622;-0.053435925;0.5711656;0.3876687;-0.14534138;-0.26627508;-0.27550367;-0.67852837;0.030087916;0.65883934;-0.37487802;-0.7232355;-0.59450394;1.1781663;-0.8523654;0.59658104;-1.1875513;0.30816957;0.93801725;-0.62147367;-0.8252599;0.18190221;-1.0157545;-0.25232163;-0.29176566;-0.11901665;-0.4040245;0.06780063;-0.94582945;-0.8967268;0.7227656;0.36309245;-1.6479182;-0.11805678;0.32280815;-0.1471284;-0.4970696;-0.51049715;-0.37756032;-0.01693507;0.09975291;-0.46334785;-0.9150675;-0.71540093;-1.6445972;0.45055994;1.2120913;0.6598703;-0.5845387;-0.7248156;-0.3406537;-0.19192709;-1.3573699;0.06528028;-0.8333389;-0.41411433;0.49934512;-0.9470191;0.17148413;0.006261951;-0.8142561;0.8113596;-1.1511804;0.104508;-0.9359585;0.15516762;-0.33596322;-0.048122827;-1.5678782;-1.8427716;0.20602855;-0.44264787;-0.31032708;0.40390447;1.1531067;-1.1003097;-0.20466933;0.68588126;0.8000297;-0.9911266;-1.3094012;0.80726016;0.61110777;-1.4939072;-0.99671406;0.57437384;0.090929605;-1.2374773;0.736153;-2.1800125;-0.7902235;0.6748452;0.29221714;0.68703943;-0.1621678;-1.0730895;0.6031117;-1.58545;0.34596545;-1.8625246;-0.53073406;0.74101496;0.65389;0.3812936;-0.07169047;-0.97831345;0.5403425;0.4651145;-0.52683794;-1.2112619;-0.6166816;-0.10079314;1.3180608;0.4882683;0.44740295;-0.85653543;-0.107200816;-0.7712045;1.031362;-0.7955005;0.15063553;-0.3595464;-0.948605;1.6070462;1.3403572;-0.35564575;-1.7131515;1.9237846\r\"): c:/program files (x86)/go agent/pipelines/buildmaster/catboost.git/catboost/libs/data/cb_dsv_loader.cpp:139: Sub-field #1023 of numeric vector for feature 1 cannot be parsed as float. Check data contents or column description"
     ]
    }
   ],
   "source": [
    "from catboost import Pool\n",
    "train_embed_pool = Pool('train_embed_text.tsv', column_description='pool_text.cd')\n",
    "test_embed_pool = Pool('test_embed_text.tsv', column_description='pool_text.cd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_embed_pool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-a5bf0110e462>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_text_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_embed_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_embed_pool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_embed_pool' is not defined"
     ]
    }
   ],
   "source": [
    "model_text_embeddings = fit_model(train_embed_pool, test_embed_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score_diff(model, model_text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
